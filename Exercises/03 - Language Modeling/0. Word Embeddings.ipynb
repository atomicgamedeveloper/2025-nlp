{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2252f27a",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561fcd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import tokenize, simple_preprocess\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown, stopwords\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from typing import Any\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.classification import BinaryAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4427ed",
   "metadata": {},
   "source": [
    "## Exercise 1: Training a Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97858931",
   "metadata": {},
   "source": [
    "### Preparing the Corpus and Stopwords\n",
    "\n",
    "To train our own **Word2Vec embeddings**, we first need a text corpus.  \n",
    "Here we’ll use the **Brown Corpus**, a classic collection of English texts available through NLTK.  \n",
    "\n",
    "- We download the `brown` corpus and the list of common English **stopwords**.  \n",
    "- Stopwords (like *the*, *and*, *is*) carry little semantic meaning, so we’ll filter them out before training.  \n",
    "\n",
    "This ensures our Word2Vec model focuses on more informative words when learning embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89825ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"brown\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82edba5",
   "metadata": {},
   "source": [
    "### Exploring the Brown Corpus\n",
    "\n",
    "Before training Word2Vec, let’s take a look at the data we’ll be working with.  \n",
    "\n",
    "- The **Brown Corpus** is divided into categories (e.g., news, fiction, humor).  \n",
    "- We print the available categories and the total number of sentences in the corpus.  \n",
    "- For illustration, we also display the first few sentences so we can see how the raw data looks.  \n",
    "\n",
    "This step helps us understand the **structure and style of the text** before preprocessing and training embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5872758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some info about the corpus\n",
    "print(\"Categories:\", brown.categories())\n",
    "print(\"Total sentences:\", len(brown.sents()))\n",
    "\n",
    "# Get 100 senteces from the humor category\n",
    "dataset = brown.sents()\n",
    "\n",
    "for i in range(3):\n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a00a5c",
   "metadata": {},
   "source": [
    "### Cleaning and Preprocessing the Text\n",
    "\n",
    "Raw sentences from the Brown Corpus need to be **preprocessed** before they can be used for training Word2Vec.  \n",
    "\n",
    "Steps applied:  \n",
    "1. **Join and tokenize** each sentence into words.  \n",
    "2. **Lowercasing & deaccenting** (`deacc=True` removes punctuation/accents).  \n",
    "3. **Filter short words** (`min_len=2` keeps words with at least 2 characters).  \n",
    "4. **Remove stopwords** (like *the*, *and*, *is*) to focus on meaningful content.  \n",
    "\n",
    "The result is a list of cleaned, tokenized sentences.  \n",
    "We print a few examples to see how the preprocessing transforms the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08fbee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sentences = ...\n",
    "\n",
    "for i in range(3):\n",
    "    print(cleaned_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae961c5",
   "metadata": {},
   "source": [
    "### Training a Word2Vec Model\n",
    "\n",
    "Now we train our own **Word2Vec embeddings** on the Brown Corpus.  \n",
    "\n",
    "Key parameters:  \n",
    "- `vector_size=50`: each word is represented as a 50-dimensional vector.  \n",
    "- `window=3`: the model looks at 3 words to the left and right for context.  \n",
    "- `min_count=1`: keep all words (even rare ones).  \n",
    "- `sg=1`: use the **skip-gram** approach, which works well for smaller datasets.  \n",
    "- `epochs=20`: make multiple passes over the data to learn stronger embeddings.  \n",
    "\n",
    "After training, we can:  \n",
    "- Inspect the learned vector for a specific word (here, `\"engineer\"`).  \n",
    "- Find its **nearest neighbors** in the embedding space, i.e., words with similar meanings or usage contexts.  \n",
    "\n",
    "This demonstrates how Word2Vec captures **semantic similarity** directly from the text we trained on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09af82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(...\n",
    ")\n",
    "\n",
    "word = \"jury\"\n",
    "# Inspect a word vector\n",
    "print(f\"Vector for '{word}':\")\n",
    "print(model.wv[word][:10])   # show first 10 dimensions\n",
    "\n",
    "# Check nearest neighbors\n",
    "print(f\"\\nMost similar to '{word}':\")\n",
    "print(model.wv.most_similar(word, topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce606d9",
   "metadata": {},
   "source": [
    "## Exercise 2: Using GloVe Embeddings to train a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feffc42d",
   "metadata": {},
   "source": [
    "### Loading Pre-trained Word Embeddings\n",
    "\n",
    "Instead of learning word vectors from scratch, we can use **pre-trained embeddings** that capture semantic relationships between words.  \n",
    "Here we download the **GloVe embeddings** (Global Vectors for Word Representation) trained on a large Wikipedia + Gigaword corpus.  \n",
    "\n",
    "- Each word is mapped to a **100-dimensional vector**.  \n",
    "- Words that appear in similar contexts (e.g., *king* and *queen*) will have vectors that are close to each other in this space.  \n",
    "\n",
    "These embeddings will serve as the foundation for representing text in our binary classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d4873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe92640",
   "metadata": {},
   "source": [
    "### Loading and Preparing the Dataset\n",
    "\n",
    "For this example, we’ll use the **Sentiment140 Twitter dataset**, which contains tweets labeled for **sentiment (positive or negative)**.  \n",
    "This is a common benchmark dataset for text classification tasks.  \n",
    "\n",
    "- We load the dataset using the 🤗 **`datasets`** library.  \n",
    "- To keep the demo lightweight and fast, we only take a **subset** of the data:  \n",
    "  - 50,000 tweets for training  \n",
    "  - 10,000 tweets for testing  \n",
    "\n",
    "Finally, we print the dataset sizes to confirm our selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943edfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"adilbekovich/Sentiment140Twitter\")\n",
    "\n",
    "train = ds[\"train\"].select(range(50_000))\n",
    "test = ds[\"test\"].select(range(10_000))\n",
    "print(f\"Training set size: {len(train)}\")\n",
    "print(f\"Test set size: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c694b0f4",
   "metadata": {},
   "source": [
    "### Converting Tweets into Embeddings\n",
    "\n",
    "Machine learning models can’t directly process raw text, so we need to convert each tweet into a **numeric vector**.  \n",
    "\n",
    "We define a function `sentence_embedding` that:  \n",
    "1. **Tokenizes** the sentence into words (removing punctuation and making everything lowercase).  \n",
    "2. Looks up the **GloVe vector** for each word.  \n",
    "3. Computes the **average** of all word vectors to create a single fixed-length representation of the entire sentence.  \n",
    "   - If a sentence has no known words, we assign a zero vector.  \n",
    "\n",
    "Then we:  \n",
    "- Apply this function to every tweet in the train and test sets.  \n",
    "- Store the result in a new column called `\"embeddings\"`.  \n",
    "- Format the dataset so that `\"embeddings\"` and `\"label\"` are ready to be used as **PyTorch tensors** for training a classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9a9a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence: str, model: Any) -> np.ndarray:\n",
    "    # TODO: implement this function\n",
    "    ...\n",
    "    \n",
    "train = # TODO: Map the training set to add embeddings\n",
    "test = # TODO: Map the test set to add embeddings\n",
    "\n",
    "train.set_format(type=\"torch\", columns=[\"embeddings\", \"label\"])\n",
    "test.set_format(type=\"torch\", columns=[\"embeddings\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7999a34",
   "metadata": {},
   "source": [
    "### Building a Simple Neural Network Classifier\n",
    "\n",
    "Now that we have numeric embeddings for each tweet, we can train a **neural network** to classify sentiment.  \n",
    "\n",
    "We define a PyTorch model `SentimentClassifier` with the following structure:  \n",
    "\n",
    "1. **Input layer**: takes in the 100-dimensional GloVe embedding for each tweet.  \n",
    "2. **Hidden layer**: a fully connected layer with 256 units and a **ReLU activation**, which introduces non-linearity.  \n",
    "3. **Output layer**: a single neuron that predicts the probability of the tweet being **positive** (values between 0 and 1).  \n",
    "   - We use a **sigmoid activation** to squash the output.  \n",
    "\n",
    "This simple feed-forward network is powerful enough to learn sentiment patterns from our averaged word embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2149cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        # TODO: Define the layers and the activation functions\n",
    "        ...\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass\n",
    "        ...\n",
    "\n",
    "model = ... # TODO: Initialize the model with the correct input dimension (hint: extract dimension from glove_vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c06dd9",
   "metadata": {},
   "source": [
    "### Setting Up Training Parameters\n",
    "\n",
    "Before training the model, we need to define some key components:  \n",
    "\n",
    "- **Batch size (64)**: the number of samples processed at once before updating the model’s parameters.  \n",
    "- **Epochs (30)**: how many times the model will see the entire training dataset.  \n",
    "- **Loss function**: we use **Binary Cross-Entropy Loss (`BCELoss`)**, which is standard for binary classification tasks.  \n",
    "- **Optimizer**: we use **Stochastic Gradient Descent (SGD)** with a learning rate of 0.01 to update model weights during training.  \n",
    "\n",
    "We also wrap our dataset into **DataLoaders**:  \n",
    "- `train_dataloader`: feeds batches of tweets into the model, shuffling to avoid order bias.  \n",
    "- `test_dataloader`: used for evaluation (no shuffling needed).  \n",
    "\n",
    "This setup prepares us for the training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682cf8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "loss_fn = nn.BCELoss()\n",
    "acc_fn = BinaryAccuracy()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f2508",
   "metadata": {},
   "source": [
    "### Training the Classifier\n",
    "\n",
    "Now we train our sentiment classifier using the training data.  \n",
    "\n",
    "For each **epoch** (full pass over the training set):  \n",
    "1. **Batching**: the `DataLoader` gives us a batch of tweet embeddings and labels.  \n",
    "2. **Forward pass**: the embeddings are passed through the model to get predictions.  \n",
    "3. **Loss calculation**: compare predictions with the true labels using **binary cross-entropy**.  \n",
    "4. **Backward pass**: compute gradients of the loss with respect to model parameters.  \n",
    "5. **Optimizer step**: update model weights using **SGD**.  \n",
    "6. **Repeat** for all batches in the epoch.  \n",
    "\n",
    "At the end of each epoch, we print the **average training loss**, which shows how well the model is learning over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c49f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for i,  batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Get the inputs and labels from the batch\n",
    "        inputs = batch['embeddings']\n",
    "        labels = batch['label']\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = ... # TODO: Get model outputs\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = ... #TODO: Compute the loss\n",
    "        epoch_loss += loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        # TODO: Zero gradients, perform backward pass, and update weights\n",
    "        ...\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss/len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe9bbef",
   "metadata": {},
   "source": [
    "### Evaluating the Model on Test Data\n",
    "\n",
    "After training, we switch the model to **evaluation mode** (`model.eval()`) to test its performance.  \n",
    "\n",
    "For each batch in the test set:  \n",
    "1. **Forward pass**: compute predictions for the embeddings.  \n",
    "2. **Loss calculation**: measure how far predictions are from the true labels.  \n",
    "3. **Thresholding**: since outputs are probabilities (0–1), we assign  \n",
    "   - `1` if prediction > 0.5 (positive sentiment)  \n",
    "   - `0` otherwise (negative sentiment).  \n",
    "4. **Accuracy**: compare predictions with labels to compute the percentage of correct classifications.  \n",
    "\n",
    "Finally, we average the results across all batches and print the **test accuracy**, which tells us how well the model generalizes to unseen tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a26fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss = 0\n",
    "batch_acc = 0\n",
    "\n",
    "model.eval()\n",
    "for i,  batch in enumerate(test_dataloader):\n",
    "    # TODO: Load the inputs and the labels from the batch, then run the forward pass, compute loss, predictions and accuracy\n",
    "    # get the inputs\n",
    "    inputs = ...\n",
    "    labels = ...\n",
    "    \n",
    "    # run forward pass\n",
    "    outputs = ...\n",
    "    \n",
    "    # Compute and print loss\n",
    "    test_loss = ...\n",
    "    predictions = ...\n",
    "    \n",
    "    \n",
    "    acc = ...\n",
    "    \n",
    "    batch_loss += test_loss.item()\n",
    "    batch_acc += acc.item()\n",
    "\n",
    "test_loss = batch_loss / len(test_dataloader)\n",
    "test_acc = batch_acc / len(test_dataloader)\n",
    "\n",
    "print(f'Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f84ad6",
   "metadata": {},
   "source": [
    "## 3. MCQ\n",
    "\n",
    "### 3.1. Purpose of Word Embeddings\n",
    "\n",
    "What is the main purpose of word embeddings in NLP?\n",
    "\n",
    "A. To convert words into high-dimensional one-hot vectors<br>\n",
    "B. To map words into continuous vector spaces that capture semantic meaning<br>\n",
    "C. To remove stopwords from text before processing<br>\n",
    "D. To reduce the training time of convolutional networks<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.2. One-Hot vs. Embeddings\n",
    "\n",
    "Compared to one-hot encoding, word embeddings:\n",
    "\n",
    "A. Have the same dimensionality as the vocabulary size<br>\n",
    "B. Provide dense, low-dimensional representations that capture similarities<br>\n",
    "C. Are always manually designed by experts<br>\n",
    "D. Cannot be trained with neural networks<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.3. Word2Vec Models\n",
    "\n",
    "The Skip-gram model in Word2Vec is designed to:\n",
    "\n",
    "A. Predict the context words given a target word<br>\n",
    "B. Predict the target word given the context words<br>\n",
    "C. Cluster words into fixed categories<br>\n",
    "D. Remove rare words from the corpus<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.4. Embedding Matrix Shape\n",
    "\n",
    "In a neural network with vocabulary size $V$ and embedding dimension $d$, the embedding matrix has shape:\n",
    "\n",
    "A. $(d \\times V)$ <br>\n",
    "B. $(V \\times d)$<br>\n",
    "C. $(V \\times V)$<br>\n",
    "D. $(d \\times d)$<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.5. Semantic Relationships\n",
    "\n",
    "Word embeddings can capture analogies such as:\n",
    "\n",
    "A. king – man + woman ≈ queen<br>\n",
    "B. dog – cat + car ≈ airplane<br>\n",
    "C. apple – red + fast ≈ running<br>\n",
    "D. chair – table + sky ≈ cloud<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.6. Contextual vs. Static Embeddings\n",
    "\n",
    "How do contextual embeddings (e.g., BERT) differ from static embeddings (e.g., Word2Vec)?\n",
    "\n",
    "A. They assign the same vector to a word regardless of context<br>\n",
    "B. They assign different vectors to a word depending on its context<br>\n",
    "C. They are always lower-dimensional than static embeddings<br>\n",
    "D. They do not require pretraining on large corpora<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.7. Sparse vs. Dense Representations\n",
    "\n",
    "Compared to Bag-of-Words (BoW) vectors, neural word embeddings are:\n",
    "\n",
    "A. Higher dimensional and sparse<br>\n",
    "B. Always binary representations<br>\n",
    "C. Lower dimensional and sparse<br>\n",
    "D. Lower dimensional and dense<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.8. Semantic Similarity\n",
    "\n",
    "What is the main advantage of word embeddings over one-hot encodings?\n",
    "\n",
    "A. They guarantee perfect accuracy in classification tasks<br>\n",
    "B. They eliminate the need for training neural networks<br>\n",
    "C. They capture semantic similarity between words in vector space<br>\n",
    "D. They automatically remove stopwords from text<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.9. CBOW vs. Skip-gram\n",
    "\n",
    "The Continuous Bag of Words (CBOW) model aims to:\n",
    "\n",
    "A. Predict the target word given its surrounding context words<br>\n",
    "B. Assign unique one-hot vectors to words<br>\n",
    "C. Predict the context words given a target word<br>\n",
    "D. Cluster words into topics using SVD<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.10. Distributional Semantics\n",
    "\n",
    "The idea that “you shall know a word by the company it keeps” refers to:\n",
    "\n",
    "A. Overfitting in NLP models<br>\n",
    "B. Context-based learning of embeddings<br>\n",
    "C. Sentence segmentation<br>\n",
    "D. Stopword removal<br>\n",
    "\n",
    "**Answer:** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
