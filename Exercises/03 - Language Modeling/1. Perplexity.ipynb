{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edac6a6e",
   "metadata": {},
   "source": [
    "# Perplexity computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b2eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86360b0",
   "metadata": {},
   "source": [
    "### Defining the Training and Test Data\n",
    "\n",
    "We start by creating a **small training corpus**, which consists of a few simple sentences.  \n",
    "This corpus will serve as the \"knowledge base\" from which our model learns word probabilities.  \n",
    "\n",
    "Next, we define a **test sentence**.  \n",
    "Perplexity will be computed on this sentence to measure how well the probability distribution learned from the training corpus predicts unseen text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5191859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training corpus demonstrating perplexity computation\n",
    "training_corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog ran in the park\", \n",
    "    \"a cat likes fish\",\n",
    "    \"the mat is soft\"\n",
    "]\n",
    "\n",
    "# Test sentence for perplexity calculation\n",
    "test_sentence = \"the cat likes the mat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed3401",
   "metadata": {},
   "source": [
    "### Building a Bigram Model\n",
    "\n",
    "This block:\n",
    "- Tokenizes sentences with `<s>` and `</s>` markers.  \n",
    "- Counts **unigrams** (single words) and **bigrams** (word pairs).  \n",
    "- Prints the counts to show what the model has learned.  \n",
    "\n",
    "We also tokenize the test sentence so itâ€™s ready for probability and perplexity calculation later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7703ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and add special tokens\n",
    "def tokenize_with_boundaries(sentence: str) -> List[str]:\n",
    "    # TODO: return list of tokens with <s> at the start and </s> at the end\n",
    "    pass\n",
    "\n",
    "# Build bigram counts\n",
    "bigram_counts = defaultdict(int)\n",
    "unigram_counts = defaultdict(int)\n",
    "\n",
    "# Tokenizing and counting\n",
    "print(f\"Tokenized corpus\")\n",
    "for sentence in training_corpus:\n",
    "    tokens = tokenize_with_boundaries(sentence)\n",
    "    print(f\"\\tTokens: {tokens}\")\n",
    "    \n",
    "    # TODO: count unigrams\n",
    "    for token in tokens:\n",
    "        ...\n",
    "    \n",
    "    # TODO: count bigrams\n",
    "    for i in range(len(tokens) - 1):\n",
    "        ...\n",
    "\n",
    "print(f\"\\nUnigram counts:\")\n",
    "for word, count in sorted(unigram_counts.items()):\n",
    "    print(f\"   {word}: {count}\")\n",
    "\n",
    "print(f\"\\nBigram counts:\")\n",
    "for bigram, count in sorted(bigram_counts.items()):\n",
    "    print(f\"   {bigram}: {count}\")\n",
    "\n",
    "test_tokens = tokenize_with_boundaries(test_sentence)\n",
    "print(f\"\\nTokenized test sentence: {test_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd151043",
   "metadata": {},
   "source": [
    "### Bigram Probabilities with Add-1 Smoothing\n",
    "\n",
    "Now we compute bigram probabilities.  \n",
    "To avoid zero probabilities for unseen word pairs, we use **add-1 smoothing** (Laplace smoothing).  \n",
    "\n",
    "For each bigram in the test sentence:\n",
    "- The probability is `(count(w1, w2) + 1) / (count(w1) + |V|)`.  \n",
    "- We also compute `log2(prob)` for use in perplexity calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe2f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(unigram_counts)\n",
    "\n",
    "def get_bigram_prob_smoothed(w1: str, w2: str) -> float:\n",
    "    \"\"\"Get bigram probability with add-1 smoothing\"\"\"\n",
    "    # TODO: Implement formula (hint: slide 41)\n",
    "    numerator = ...\n",
    "    denominator = ...\n",
    "    return numerator / denominator\n",
    "\n",
    "print(f\"Computing the Bigram probabilities:\")\n",
    "log_prob_sum = 0\n",
    "n_tokens = len(test_tokens) - 1  # Number of bigrams\n",
    "\n",
    "for i in range(len(test_tokens) - 1):\n",
    "    w1, w2 = test_tokens[i], test_tokens[i + 1]\n",
    "    # TODO: compute probability and log2(prob)\n",
    "    prob = ...\n",
    "    log_prob = ...\n",
    "    log_prob_sum += ...\n",
    "    \n",
    "    print(f\"   P({w2}|{w1}) = ({bigram_counts[(w1, w2)]} + 1) / ({unigram_counts[w1]} + {vocab_size}) = {prob:.4f}\")\n",
    "    print(f\"   log2({prob:.4f}) = {log_prob:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5b3bc5",
   "metadata": {},
   "source": [
    "### Perplexity Computation\n",
    "\n",
    "Finally, we calculate **perplexity** of the test sentence under our bigram model:  \n",
    "\n",
    "\\begin{align}\n",
    "\\text{Perplexity} = 2^{-\\frac{1}{N} \\sum \\log_2 P(w_i \\mid w_{i-1})}\n",
    "\\end{align}\n",
    "\n",
    "- `N` = number of bigrams in the test sentence.  \n",
    "- We average the log probabilities and exponentiate to get perplexity.  \n",
    "\n",
    "A **lower perplexity** means the model finds the test sentence more predictable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b7501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nPerplexity calculation:\")\n",
    "print(f\"   Sum of log probabilities: {log_prob_sum:.4f}\")\n",
    "print(f\"   Number of tokens (N): {n_tokens}\")\n",
    "\n",
    "# TODO: Compute perplexity (hint: equation above + slide 30)\n",
    "perplexity = ...\n",
    "print(f\"   Perplexity = {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d27d0f",
   "metadata": {},
   "source": [
    "## MCQ\n",
    "\n",
    "### 4.1. Definition of Perplexity  \n",
    "\n",
    "What does perplexity measure in language models?  \n",
    "\n",
    "A. The total number of words in the test set<br>  \n",
    "B. The unpredictability or \"surprise\" of a model when predicting text<br>  \n",
    "C. The size of the vocabulary<br>  \n",
    "D. The average frequency of bigrams<br>  \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 4.2. Formula for Perplexity  \n",
    "\n",
    "Which of the following is the correct formula for perplexity of a test set with *N* tokens?  \n",
    "\n",
    "A. $\\text{Perplexity} = \\frac{1}{N} \\sum \\log P(w_i)$<br>  \n",
    "B. $\\text{Perplexity} = 2^{-\\frac{1}{N} \\sum \\log_2 P(w_i \\mid context)}$<br>  \n",
    "C. $\\text{Perplexity} = \\prod_{i=1}^{N} P(w_i)$<br>  \n",
    "D. $\\text{Perplexity} = N^{\\sum P(w_i)}$<br>  \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 4.3. Interpretation of Perplexity  \n",
    "\n",
    "If a model has **lower perplexity** on a test set, what does it mean?  \n",
    "\n",
    "A. The model is more confident and better at predicting the test data<br>  \n",
    "B. The model is overfitting<br>  \n",
    "C. The vocabulary size is smaller<br>  \n",
    "D. The training corpus is too simple<br>  \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 4.4. Smoothing and Perplexity  \n",
    "\n",
    "Why is **add-1 smoothing (Laplace smoothing)** used when computing perplexity?  \n",
    "\n",
    "A. To reduce the vocabulary size<br>  \n",
    "B. To ensure unseen word pairs do not get zero probability<br>  \n",
    "C. To increase the average log probability<br>  \n",
    "D. To make the model faster to train<br>  \n",
    "\n",
    "**Answer:**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
