{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2252f27a",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "561fcd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\malik\\miniconda3\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (1.8.2)\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from torchmetrics) (1.26.4)\n",
      "Requirement already satisfied: packaging>17.1 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from torchmetrics) (25.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from torchmetrics) (2.9.0.dev20250831+cu129)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from torchmetrics) (0.15.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (78.1.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.19.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\malik\\miniconda3\\envs\\nlp\\lib\\site-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import tokenize, simple_preprocess\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown, stopwords\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from typing import Any\n",
    "\n",
    "%pip install torchmetrics\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.classification import BinaryAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4427ed",
   "metadata": {},
   "source": [
    "## Exercise 1: Training a Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97858931",
   "metadata": {},
   "source": [
    "### Preparing the Corpus and Stopwords\n",
    "\n",
    "To train our own **Word2Vec embeddings**, we first need a text corpus.  \n",
    "Here we’ll use the **Brown Corpus**, a classic collection of English texts available through NLTK.  \n",
    "\n",
    "- We download the `brown` corpus and the list of common English **stopwords**.  \n",
    "- Stopwords (like *the*, *and*, *is*) carry little semantic meaning, so we’ll filter them out before training.  \n",
    "\n",
    "This ensures our Word2Vec model focuses on more informative words when learning embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89825ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\malik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\malik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"brown\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82edba5",
   "metadata": {},
   "source": [
    "### Exploring the Brown Corpus\n",
    "\n",
    "Before training Word2Vec, let’s take a look at the data we’ll be working with.  \n",
    "\n",
    "- The **Brown Corpus** is divided into categories (e.g., news, fiction, humor).  \n",
    "- We print the available categories and the total number of sentences in the corpus.  \n",
    "- For illustration, we also display the first few sentences so we can see how the raw data looks.  \n",
    "\n",
    "This step helps us understand the **structure and style of the text** before preprocessing and training embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5872758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "Total sentences: 57340\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n",
      "['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']\n"
     ]
    }
   ],
   "source": [
    "# Show some info about the corpus\n",
    "print(\"Categories:\", brown.categories())\n",
    "print(\"Total sentences:\", len(brown.sents()))\n",
    "\n",
    "# Get 100 senteces from the humor category\n",
    "dataset = brown.sents()\n",
    "\n",
    "for i in range(3):\n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a00a5c",
   "metadata": {},
   "source": [
    "### Cleaning and Preprocessing the Text\n",
    "\n",
    "Raw sentences from the Brown Corpus need to be **preprocessed** before they can be used for training Word2Vec.  \n",
    "\n",
    "Steps applied:  \n",
    "1. **Join and tokenize** each sentence into words.  \n",
    "2. **Lowercasing & deaccenting** (`deacc=True` removes punctuation/accents).  \n",
    "3. **Filter short words** (`min_len=2` keeps words with at least 2 characters).  \n",
    "4. **Remove stopwords** (like *the*, *and*, *is*) to focus on meaningful content.  \n",
    "\n",
    "The result is a list of cleaned, tokenized sentences.  \n",
    "We print a few examples to see how the preprocessing transforms the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e08fbee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investigation', 'atlanta', 'recent', 'primary', 'election', 'produced', 'evidence', 'irregularities', 'took', 'place'], ['jury', 'said', 'term', 'end', 'presentments', 'city', 'executive', 'committee', 'charge', 'election', 'deserves', 'praise', 'thanks', 'city', 'atlanta', 'manner', 'election', 'conducted'], ['september', 'october', 'term', 'jury', 'charged', 'fulton', 'superior', 'court', 'judge', 'durwood', 'pye', 'investigate', 'reports', 'possible', 'irregularities', 'hard', 'fought', 'primary', 'mayor', 'nominate', 'ivan', 'allen', 'jr'], ['relative', 'handful', 'reports', 'received', 'jury', 'said', 'considering', 'widespread', 'interest', 'election', 'number', 'voters', 'size', 'city'], ['jury', 'said', 'find', 'many', 'georgia', 'registration', 'election', 'laws', 'outmoded', 'inadequate', 'often', 'ambiguous'], ['recommended', 'fulton', 'legislators', 'act', 'laws', 'studied', 'revised', 'end', 'modernizing', 'improving'], ['grand', 'jury', 'commented', 'number', 'topics', 'among', 'atlanta', 'fulton', 'county', 'purchasing', 'departments', 'said', 'well', 'operated', 'follow', 'generally', 'accepted', 'practices', 'inure', 'best', 'interest', 'governments'], ['merger', 'proposed'], ['however', 'jury', 'said', 'believes', 'two', 'offices', 'combined', 'achieve', 'greater', 'efficiency', 'reduce', 'cost', 'administration'], ['city', 'purchasing', 'department', 'jury', 'said', 'lacking', 'experienced', 'clerical', 'personnel', 'result', 'city', 'personnel', 'policies'], ['urged', 'city', 'take', 'steps', 'remedy', 'problem'], ['implementation', 'georgia', 'automobile', 'title', 'law', 'also', 'recommended', 'outgoing', 'jury'], ['urged', 'next', 'legislature', 'provide', 'enabling', 'funds', 'set', 'effective', 'date', 'orderly', 'implementation', 'law', 'may', 'effected'], ['grand', 'jury', 'took', 'swipe', 'state', 'welfare', 'department', 'handling', 'federal', 'funds', 'granted', 'child', 'welfare', 'services', 'foster', 'homes'], ['one', 'major', 'items', 'fulton', 'county', 'general', 'assistance', 'program', 'jury', 'said', 'state', 'welfare', 'department', 'seen', 'fit', 'distribute', 'funds', 'welfare', 'departments', 'counties', 'state', 'exception', 'fulton', 'county', 'receives', 'none', 'money'], ['jurors', 'said', 'realize', 'proportionate', 'distribution', 'funds', 'might', 'disable', 'program', 'less', 'populous', 'counties'], ['nevertheless', 'feel', 'future', 'fulton', 'county', 'receive', 'portion', 'available', 'funds', 'jurors', 'said'], ['failure', 'continue', 'place', 'burden', 'fulton', 'taxpayers'], ['jury', 'also', 'commented', 'fulton', 'ordinary', 'court', 'fire', 'practices', 'appointment', 'appraisers', 'guardians', 'administrators', 'awarding', 'fees', 'compensation'], ['wards', 'protected']]\n"
     ]
    }
   ],
   "source": [
    "cleaned_sentences = []\n",
    "for i in range(len(dataset)):\n",
    "    cleaned_sentence = \" \".join(dataset[i])\n",
    "    cleaned_sentence = simple_preprocess(cleaned_sentence,True,2)\n",
    "    cleaned_sentence = [token for token in cleaned_sentence if not token in stop_words]\n",
    "    cleaned_sentences.append(cleaned_sentence)\n",
    "\n",
    "print(cleaned_sentences[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae961c5",
   "metadata": {},
   "source": [
    "### Training a Word2Vec Model\n",
    "\n",
    "Now we train our own **Word2Vec embeddings** on the Brown Corpus.  \n",
    "\n",
    "Key parameters:  \n",
    "- `vector_size=50`: each word is represented as a 50-dimensional vector.  \n",
    "- `window=3`: the model looks at 3 words to the left and right for context.  \n",
    "- `min_count=1`: keep all words (even rare ones).  \n",
    "- `sg=1`: use the **skip-gram** approach, which works well for smaller datasets.  \n",
    "- `epochs=20`: make multiple passes over the data to learn stronger embeddings.  \n",
    "\n",
    "After training, we can:  \n",
    "- Inspect the learned vector for a specific word (here, `\"engineer\"`).  \n",
    "- Find its **nearest neighbors** in the embedding space, i.e., words with similar meanings or usage contexts.  \n",
    "\n",
    "This demonstrates how Word2Vec captures **semantic similarity** directly from the text we trained on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09af82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(...\n",
    ")\n",
    "\n",
    "word = \"jury\"\n",
    "# Inspect a word vector\n",
    "print(f\"Vector for '{word}':\")\n",
    "print(model.wv[word][:10])   # show first 10 dimensions\n",
    "\n",
    "# Check nearest neighbors\n",
    "print(f\"\\nMost similar to '{word}':\")\n",
    "print(model.wv.most_similar(word, topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce606d9",
   "metadata": {},
   "source": [
    "## Exercise 2: Using GloVe Embeddings to train a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feffc42d",
   "metadata": {},
   "source": [
    "### Loading Pre-trained Word Embeddings\n",
    "\n",
    "Instead of learning word vectors from scratch, we can use **pre-trained embeddings** that capture semantic relationships between words.  \n",
    "Here we download the **GloVe embeddings** (Global Vectors for Word Representation) trained on a large Wikipedia + Gigaword corpus.  \n",
    "\n",
    "- Each word is mapped to a **100-dimensional vector**.  \n",
    "- Words that appear in similar contexts (e.g., *king* and *queen*) will have vectors that are close to each other in this space.  \n",
    "\n",
    "These embeddings will serve as the foundation for representing text in our binary classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d4873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe92640",
   "metadata": {},
   "source": [
    "### Loading and Preparing the Dataset\n",
    "\n",
    "For this example, we’ll use the **Sentiment140 Twitter dataset**, which contains tweets labeled for **sentiment (positive or negative)**.  \n",
    "This is a common benchmark dataset for text classification tasks.  \n",
    "\n",
    "- We load the dataset using the 🤗 **`datasets`** library.  \n",
    "- To keep the demo lightweight and fast, we only take a **subset** of the data:  \n",
    "  - 50,000 tweets for training  \n",
    "  - 10,000 tweets for testing  \n",
    "\n",
    "Finally, we print the dataset sizes to confirm our selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943edfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"adilbekovich/Sentiment140Twitter\")\n",
    "\n",
    "train = ds[\"train\"].select(range(50_000))\n",
    "test = ds[\"test\"].select(range(10_000))\n",
    "print(f\"Training set size: {len(train)}\")\n",
    "print(f\"Test set size: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c694b0f4",
   "metadata": {},
   "source": [
    "### Converting Tweets into Embeddings\n",
    "\n",
    "Machine learning models can’t directly process raw text, so we need to convert each tweet into a **numeric vector**.  \n",
    "\n",
    "We define a function `sentence_embedding` that:  \n",
    "1. **Tokenizes** the sentence into words (removing punctuation and making everything lowercase).  \n",
    "2. Looks up the **GloVe vector** for each word.  \n",
    "3. Computes the **average** of all word vectors to create a single fixed-length representation of the entire sentence.  \n",
    "   - If a sentence has no known words, we assign a zero vector.  \n",
    "\n",
    "Then we:  \n",
    "- Apply this function to every tweet in the train and test sets.  \n",
    "- Store the result in a new column called `\"embeddings\"`.  \n",
    "- Format the dataset so that `\"embeddings\"` and `\"label\"` are ready to be used as **PyTorch tensors** for training a classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9a9a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence: str, model: Any) -> np.ndarray:\n",
    "    # TODO: implement this function\n",
    "    ...\n",
    "    \n",
    "train = # TODO: Map the training set to add embeddings\n",
    "test = # TODO: Map the test set to add embeddings\n",
    "\n",
    "train.set_format(type=\"torch\", columns=[\"embeddings\", \"label\"])\n",
    "test.set_format(type=\"torch\", columns=[\"embeddings\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7999a34",
   "metadata": {},
   "source": [
    "### Building a Simple Neural Network Classifier\n",
    "\n",
    "Now that we have numeric embeddings for each tweet, we can train a **neural network** to classify sentiment.  \n",
    "\n",
    "We define a PyTorch model `SentimentClassifier` with the following structure:  \n",
    "\n",
    "1. **Input layer**: takes in the 100-dimensional GloVe embedding for each tweet.  \n",
    "2. **Hidden layer**: a fully connected layer with 256 units and a **ReLU activation**, which introduces non-linearity.  \n",
    "3. **Output layer**: a single neuron that predicts the probability of the tweet being **positive** (values between 0 and 1).  \n",
    "   - We use a **sigmoid activation** to squash the output.  \n",
    "\n",
    "This simple feed-forward network is powerful enough to learn sentiment patterns from our averaged word embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2149cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        # TODO: Define the layers and the activation functions\n",
    "        ...\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass\n",
    "        ...\n",
    "\n",
    "model = ... # TODO: Initialize the model with the correct input dimension (hint: extract dimension from glove_vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c06dd9",
   "metadata": {},
   "source": [
    "### Setting Up Training Parameters\n",
    "\n",
    "Before training the model, we need to define some key components:  \n",
    "\n",
    "- **Batch size (64)**: the number of samples processed at once before updating the model’s parameters.  \n",
    "- **Epochs (30)**: how many times the model will see the entire training dataset.  \n",
    "- **Loss function**: we use **Binary Cross-Entropy Loss (`BCELoss`)**, which is standard for binary classification tasks.  \n",
    "- **Optimizer**: we use **Stochastic Gradient Descent (SGD)** with a learning rate of 0.01 to update model weights during training.  \n",
    "\n",
    "We also wrap our dataset into **DataLoaders**:  \n",
    "- `train_dataloader`: feeds batches of tweets into the model, shuffling to avoid order bias.  \n",
    "- `test_dataloader`: used for evaluation (no shuffling needed).  \n",
    "\n",
    "This setup prepares us for the training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682cf8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "loss_fn = nn.BCELoss()\n",
    "acc_fn = BinaryAccuracy()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f2508",
   "metadata": {},
   "source": [
    "### Training the Classifier\n",
    "\n",
    "Now we train our sentiment classifier using the training data.  \n",
    "\n",
    "For each **epoch** (full pass over the training set):  \n",
    "1. **Batching**: the `DataLoader` gives us a batch of tweet embeddings and labels.  \n",
    "2. **Forward pass**: the embeddings are passed through the model to get predictions.  \n",
    "3. **Loss calculation**: compare predictions with the true labels using **binary cross-entropy**.  \n",
    "4. **Backward pass**: compute gradients of the loss with respect to model parameters.  \n",
    "5. **Optimizer step**: update model weights using **SGD**.  \n",
    "6. **Repeat** for all batches in the epoch.  \n",
    "\n",
    "At the end of each epoch, we print the **average training loss**, which shows how well the model is learning over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c49f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for i,  batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Get the inputs and labels from the batch\n",
    "        inputs = batch['embeddings']\n",
    "        labels = batch['label']\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = ... # TODO: Get model outputs\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = ... #TODO: Compute the loss\n",
    "        epoch_loss += loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        # TODO: Zero gradients, perform backward pass, and update weights\n",
    "        ...\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss/len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe9bbef",
   "metadata": {},
   "source": [
    "### Evaluating the Model on Test Data\n",
    "\n",
    "After training, we switch the model to **evaluation mode** (`model.eval()`) to test its performance.  \n",
    "\n",
    "For each batch in the test set:  \n",
    "1. **Forward pass**: compute predictions for the embeddings.  \n",
    "2. **Loss calculation**: measure how far predictions are from the true labels.  \n",
    "3. **Thresholding**: since outputs are probabilities (0–1), we assign  \n",
    "   - `1` if prediction > 0.5 (positive sentiment)  \n",
    "   - `0` otherwise (negative sentiment).  \n",
    "4. **Accuracy**: compare predictions with labels to compute the percentage of correct classifications.  \n",
    "\n",
    "Finally, we average the results across all batches and print the **test accuracy**, which tells us how well the model generalizes to unseen tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a26fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss = 0\n",
    "batch_acc = 0\n",
    "\n",
    "model.eval()\n",
    "for i,  batch in enumerate(test_dataloader):\n",
    "    # TODO: Load the inputs and the labels from the batch, then run the forward pass, compute loss, predictions and accuracy\n",
    "    # get the inputs\n",
    "    inputs = ...\n",
    "    labels = ...\n",
    "    \n",
    "    # run forward pass\n",
    "    outputs = ...\n",
    "    \n",
    "    # Compute and print loss\n",
    "    test_loss = ...\n",
    "    predictions = ...\n",
    "    \n",
    "    \n",
    "    acc = ...\n",
    "    \n",
    "    batch_loss += test_loss.item()\n",
    "    batch_acc += acc.item()\n",
    "\n",
    "test_loss = batch_loss / len(test_dataloader)\n",
    "test_acc = batch_acc / len(test_dataloader)\n",
    "\n",
    "print(f'Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f84ad6",
   "metadata": {},
   "source": [
    "## 3. MCQ\n",
    "\n",
    "### 3.1. Purpose of Word Embeddings\n",
    "\n",
    "What is the main purpose of word embeddings in NLP?\n",
    "\n",
    "A. To convert words into high-dimensional one-hot vectors<br>\n",
    "B. To map words into continuous vector spaces that capture semantic meaning<br>\n",
    "C. To remove stopwords from text before processing<br>\n",
    "D. To reduce the training time of convolutional networks<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.2. One-Hot vs. Embeddings\n",
    "\n",
    "Compared to one-hot encoding, word embeddings:\n",
    "\n",
    "A. Have the same dimensionality as the vocabulary size<br>\n",
    "B. Provide dense, low-dimensional representations that capture similarities<br>\n",
    "C. Are always manually designed by experts<br>\n",
    "D. Cannot be trained with neural networks<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.3. Word2Vec Models\n",
    "\n",
    "The Skip-gram model in Word2Vec is designed to:\n",
    "\n",
    "A. Predict the context words given a target word<br>\n",
    "B. Predict the target word given the context words<br>\n",
    "C. Cluster words into fixed categories<br>\n",
    "D. Remove rare words from the corpus<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.4. Embedding Matrix Shape\n",
    "\n",
    "In a neural network with vocabulary size $V$ and embedding dimension $d$, the embedding matrix has shape:\n",
    "\n",
    "A. $(d \\times V)$ <br>\n",
    "B. $(V \\times d)$<br>\n",
    "C. $(V \\times V)$<br>\n",
    "D. $(d \\times d)$<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.5. Semantic Relationships\n",
    "\n",
    "Word embeddings can capture analogies such as:\n",
    "\n",
    "A. king – man + woman ≈ queen<br>\n",
    "B. dog – cat + car ≈ airplane<br>\n",
    "C. apple – red + fast ≈ running<br>\n",
    "D. chair – table + sky ≈ cloud<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.6. Contextual vs. Static Embeddings\n",
    "\n",
    "How do contextual embeddings (e.g., BERT) differ from static embeddings (e.g., Word2Vec)?\n",
    "\n",
    "A. They assign the same vector to a word regardless of context<br>\n",
    "B. They assign different vectors to a word depending on its context<br>\n",
    "C. They are always lower-dimensional than static embeddings<br>\n",
    "D. They do not require pretraining on large corpora<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.7. Sparse vs. Dense Representations\n",
    "\n",
    "Compared to Bag-of-Words (BoW) vectors, neural word embeddings are:\n",
    "\n",
    "A. Higher dimensional and sparse<br>\n",
    "B. Always binary representations<br>\n",
    "C. Lower dimensional and sparse<br>\n",
    "D. Lower dimensional and dense<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.8. Semantic Similarity\n",
    "\n",
    "What is the main advantage of word embeddings over one-hot encodings?\n",
    "\n",
    "A. They guarantee perfect accuracy in classification tasks<br>\n",
    "B. They eliminate the need for training neural networks<br>\n",
    "C. They capture semantic similarity between words in vector space<br>\n",
    "D. They automatically remove stopwords from text<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.9. CBOW vs. Skip-gram\n",
    "\n",
    "The Continuous Bag of Words (CBOW) model aims to:\n",
    "\n",
    "A. Predict the target word given its surrounding context words<br>\n",
    "B. Assign unique one-hot vectors to words<br>\n",
    "C. Predict the context words given a target word<br>\n",
    "D. Cluster words into topics using SVD<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.10. Distributional Semantics\n",
    "\n",
    "The idea that “you shall know a word by the company it keeps” refers to:\n",
    "\n",
    "A. Overfitting in NLP models<br>\n",
    "B. Context-based learning of embeddings<br>\n",
    "C. Sentence segmentation<br>\n",
    "D. Stopword removal<br>\n",
    "\n",
    "**Answer:** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
