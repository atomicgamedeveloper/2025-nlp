{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc12ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Q: Queries (shape: [n_queries, d_k])\n",
    "    K: Keys    (shape: [n_keys, d_k])\n",
    "    V: Values  (shape: [n_keys, d_v])\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)  # key dimension\n",
    "    \n",
    "    # 1. Raw scores = Q @ K^T\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    print(f\"Raw scores: {scores}\")\n",
    "    \n",
    "    # 2. Scale\n",
    "    scaled_scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    print(f\"Scaled scores: {scaled_scores}\")\n",
    "    \n",
    "    # 3. Softmax along keys axis\n",
    "    weights = F.softmax(scaled_scores, dim=-1)\n",
    "    print(f\"Attention weights: {weights}\")\n",
    "    \n",
    "    # 4. Weighted sum of values\n",
    "    output = torch.matmul(weights, V)\n",
    "    print(f\"Final output: {output}\")\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "# Example sentence: \"He sat by the river bank\"\n",
    "words = [\"He\", \"sat\", \"river\", \"bank\"]\n",
    "\n",
    "Q = torch.tensor([[1.0, 0.0]])   # the query vector for \"bank\"\n",
    "K = torch.tensor([\n",
    "    [1.0, 0.0],  # \"He\"\n",
    "    [0.0, 1.0],  # \"sat\"\n",
    "    [1.0, 1.0],  # \"river\"\n",
    "    [1.0, 0.0],  # \"bank\"\n",
    "])\n",
    "V = torch.tensor([\n",
    "    [1.0, 0.0],  # info from \"He\"\n",
    "    [0.0, 1.0],  # info from \"sat\"\n",
    "    [0.5, 0.5],  # info from \"river\"\n",
    "    [1.0, 0.0],  # info from \"bank\"\n",
    "])\n",
    "\n",
    "print('Sentence: \"He sat by the river bank\"')\n",
    "print('Query: \"bank\" looking for relevant context\\n')\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"\\nAttention distribution:\")\n",
    "for word, weight in zip(words, weights[0]):\n",
    "    print(f\"{word}: {weight:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
