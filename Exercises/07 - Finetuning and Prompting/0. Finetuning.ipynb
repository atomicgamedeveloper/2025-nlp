{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f6a6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding, DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import evaluate\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78866f4d",
   "metadata": {},
   "source": [
    "# Task-A: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7477fdfa",
   "metadata": {},
   "source": [
    "## A.1 Pretrained model\n",
    "In this subsection we will load a pretrained model and evaluate its performance on a movie, sentiment analysis task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6f7790",
   "metadata": {},
   "source": [
    "In Hugging Face’s Transformers library, the `Auto Classes` are generic wrappers that automatically pick the right model/tokenizer/config class for you, based on the pretrained model you’re loading.\n",
    "\n",
    "### AutoTokenizer\n",
    "- `AutoTokenizer` is a Hugging Face class that automatically picks the right tokenizer for the model you specify.\n",
    "- A tokenizer is responsible for splitting text into tokens (subwords or word pieces) that the model can understand.\n",
    "- `AutoTokenizer.from_pretrained(model_name)` downloads and loads the pretrained tokenizer for the model of your choice.\n",
    "\n",
    "### AutoModelForSequenceClassification\n",
    "- `AutoModelForSequenceClassification` is a Hugging Face class that loads a model configured for sequence classification (e.g., sentiment analysis, text classification).\n",
    "- `AutoModelForSequenceClassification.from_pretrained(model_name)`This method loads a model that has already been trained (pretrained) and published.\n",
    "\n",
    "This is the setup we’re using for the following exercise. In practice, Hugging Face provides many additional classes for a wide variety of setups. You are encouraged to explore the [Auto Classes documentation](https://huggingface.co/docs/transformers/en/model_doc/auto) to get a broader understanding of what’s available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4fc6fc",
   "metadata": {},
   "source": [
    "**`TODO:`** Load the tokenizer and the pretrained model for sequence classification for `distilbert/distilbert-base-uncased`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4173b5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6e2839f",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "- The datasets library is another core library from Hugging Face, separate from transformers.\n",
    "- In short, this library is designed to make it easy to access, share, preprocess, and work with large datasets (especially for NLP, but also vision, audio, and multimodal tasks).\n",
    "- The `load_dataset(\"dataset_name\")` function pulls and prepares a dataset from the internet.\n",
    "- The `load_from_disk(\"dataset_name\")` function loads a local dataset in the same way.\n",
    "- The `.map()` method applies a function to every element (or batch of elements) in a Dataset. Have a look at an example here: [link](https://huggingface.co/docs/datasets/en/process#map).\n",
    "- When loading the dataset using the above function, the `split` argument can be used to get a specific split.\n",
    "- Note: These are different from the PyTorch datasets. They're similar and it's easy to transition from one to the other but they're not identical.\n",
    "\n",
    "For further documentation regarding HF dataset, you are encouraged to explore the following [documentation](https://huggingface.co/docs/datasets/en/index)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b520005",
   "metadata": {},
   "source": [
    "**`TODO:`** Load the train and test split of the `imdb` dataset. How many samples are in each split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd452673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66f56d8d",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "- `DataLoader` is a PyTorch utility that wraps a dataset and handles batching, shuffling, and parallel loading.\n",
    "- It takes a dataset (can be from both HF and PyTorch) and returns an iterator you can loop over in training or evaluation.\n",
    "\n",
    "Have a look at its [documentation](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). Specifically, see what arguments are need to load a dataset, to set your own batch size and decide whether the data will be shuffled or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148e42df",
   "metadata": {},
   "source": [
    "**`TODO:`** Load your test data into a `DataLoader` of batch size 16. Do not shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d70e7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d928d447",
   "metadata": {},
   "source": [
    "**`TODO:`** As we have previously mentioned, `DataLoader` returns an iterator. Using a `for` loop, investigate what the iterator returns for its first iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a82fda2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf941c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate a Hugging Face sequence classification model on a test dataset.\n",
    "\n",
    "    Parameters:\n",
    "        model (transformers.AutoModelForSequenceClassification): The pretrained sequence classification model to evaluate.\n",
    "        tokenizer (transformers.AutoTokenizer): The tokenizer used to preprocess the input text for the model.\n",
    "        test_loader (torch.utils.data.DataLoader): A DataLoader providing the test dataset.\n",
    "\n",
    "    Returns:\n",
    "        all_preds (torch.Tensor): Model predictions on the test set.\n",
    "        all_labels (torch.Tensor): Ground truth labels from the test set.\n",
    "        acc (float): Overall accuracy on the test set.\n",
    "        f1 (float): F1 score on the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    all_labels = None\n",
    "    all_preds = None\n",
    "\n",
    "    for batch in tqdm(test_loader):\n",
    "        text = batch['text']\n",
    "        labels = batch['label']\n",
    "\n",
    "        # TODO: Tokenize the text using the provided tokenizer, use both truncation and padding.\n",
    "        inputs = ...\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.to('cuda')\n",
    "            model = model.to('cuda')\n",
    "\n",
    "        # TODO: Perform a forward pass through the output of the model\n",
    "        with torch.no_grad():\n",
    "            outputs = ...\n",
    "        \n",
    "        # TODO: Get the logits from the model's output and compute the predictions by taking the argmax\n",
    "        logits = ...\n",
    "        preds = ...\n",
    "\n",
    "        if all_labels is None:\n",
    "            all_labels = labels.cpu()\n",
    "            all_preds = preds.cpu()\n",
    "        else:\n",
    "            all_labels = torch.cat((all_labels, labels.cpu()))\n",
    "            all_preds = torch.cat((all_preds, preds.cpu()))\n",
    "\n",
    "    # TODO: compute f1 score between model predictions and ground-truth labels (you can use sklearn.metrics)\n",
    "    f1 = ...\n",
    "\n",
    "    # TODO: compute accuracy score between model predictions and ground-truth labels (you can use sklearn.metrics)\n",
    "    acc = ...\n",
    "\n",
    "    # TODO: compute the accuracy on Positive(label==1) samples\n",
    "    pos_acc = ...\n",
    "\n",
    "    # TODO: compute the accuracy on Negative(label==0) samples\n",
    "    neg_acc = ...\n",
    "\n",
    "    print('Accuracy: ', acc*100, '%')\n",
    "    print(' -- Positive Accuracy: ', pos_acc*100, '%')\n",
    "    print(' -- Negative Accuracy: ', neg_acc*100, '%')\n",
    "    print('F1 score: ', f1)\n",
    "\n",
    "    return all_preds, all_labels, acc, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f7acb1",
   "metadata": {},
   "source": [
    "## A.2 Finetuned model\n",
    "In this section, we will further train the model for the task that we are interested in and see if we can increase its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94071258",
   "metadata": {},
   "source": [
    "**`TODO:`** Use `evaluate_model` to measure the performance of the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e03edd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "578f260f",
   "metadata": {},
   "source": [
    "**`TODO:`** Define a function that receives some samples and then uses the tokenizer we have defined to tokenize the samples. Use the `Dataset.map`method that we have previously discussed to apply your function to the train data. Keep this in mind when defining the tokenizing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c6e0c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9b876d3",
   "metadata": {},
   "source": [
    "### Data Collator\n",
    "- A data collator is a small function/class that tells the DataLoader how to merge a list of individual samples into a single batch.\n",
    "- In NLP, a main challenge is padding. Since sentences have different lengths, you need to pad them so they fit into a uniform tensor batch.\n",
    "- The `DataCollatorWithPadding` will  automatically pad sequences in a batch to the length of the longest sequence in that batch (dynamic padding) based on the tokenizer you're using.\n",
    "\n",
    "For more info on Data Collators please refer to the following [documentation](https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.DataCollatorWithPadding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18043488",
   "metadata": {},
   "source": [
    "**`TODO:`** Define a data collator that automatically pads sequences in a batch based on the defined `tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661faa3a",
   "metadata": {},
   "source": [
    "### Training Arguments\n",
    "- A configuration object that stores all the knobs and settings related to the training procedure (like learning rate, batch size, number of epochs, output directory, etc.).\n",
    "- It tells the training loop how to train (e.g., optimization settings, saving/checkpoint rules, logging options).\n",
    "- You don’t train with it directly, you just define the \"rules of training.\"\n",
    "\n",
    "### Trainer\n",
    "- This is the high-level training loop: it actually runs the training, evaluation, and prediction based on your model, data, and `TrainingArguments`.\n",
    "- It takes care of all the heavy lifting (forward pass, loss calculation, backprop, optimizer steps, checkpoint saving, etc.).\n",
    "- You just call methods like `.train()`, `.evaluate()`, or `.predict()` without writing a manual loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75570a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_name = \"imdb-finetuned-distilbert\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir_name,          # Where to save model checkpoints and logs\n",
    "    learning_rate=2e-5,                  # Step size for the optimizer (how fast the model learns)\n",
    "    per_device_train_batch_size=16,      # Training batch size per GPU/CPU device\n",
    "    per_device_eval_batch_size=16,       # Evaluation batch size per GPU/CPU device\n",
    "    num_train_epochs=1,                  # Number of times to iterate over the full training dataset\n",
    "    weight_decay=0.01,                   # Strength of L2 regularization (helps prevent overfitting)\n",
    "    save_strategy=\"epoch\",               # When to save checkpoints (\"epoch\" = at the end of each epoch)\n",
    "    push_to_hub=False,                   # Whether to push the model to the Hugging Face Hub\n",
    "    report_to=\"none\"                     # Where to report logs (e.g., \"wandb\", \"tensorboard\", \"none\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773061ac",
   "metadata": {},
   "source": [
    "**`TODO:`** Based on everything what we have defined so far in this exercise, complete the following code to initialize the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102a3f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaf6909f",
   "metadata": {},
   "source": [
    "**`TODO:`** Use the `.train` method of the trainer to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893a0220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89ad3eb4",
   "metadata": {},
   "source": [
    "**`TODO:`** Use `evaluate_model` to measure the performance of the post-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9448c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "672e5130",
   "metadata": {},
   "source": [
    "# Task-B: Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fba497e",
   "metadata": {},
   "source": [
    "### `text_target` in Tokenizers\n",
    "\n",
    "- The text_target argument is used when working with sequence-to-sequence (encoder–decoder) models such as T5, BART, mBART, or mT5.\n",
    "- It allows you to tokenize the target/output text (e.g. a translation or summary) alongside the input text in a single call to the tokenizer.\n",
    "- The tokenized targets are stored under the key labels, which the model uses during training for loss computation.\n",
    "\n",
    "### AutoModelForSeq2SeqLM\n",
    "- `AutoModelForSeq2SeqLM` is a Hugging Face class that loads a model configured for sequence-to-sequence tasks (e.g., machine translation, text summarization, question answering, text generation with input-output pairs).\n",
    "- These models typically take in a sequence as input (e.g., a sentence or paragraph) and generate a new sequence as output (e.g., a translated or summarized version).\n",
    "\n",
    "Note: The specific tokenizer will require you to `pip install protobuf`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ed78e",
   "metadata": {},
   "source": [
    "**`TODO:`** Load the tokenizer and the pretrained model for sequence-to-sequence tasks for `google/mt5-small`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55145b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "840b59b0",
   "metadata": {},
   "source": [
    "**`TODO:`** Use the tokenizer to tokenize both an input and a target in one go. You can use \"Hello\" for input and \"Bonjour\" for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5928716a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b624362",
   "metadata": {},
   "source": [
    "**`TODO:`** Unzip the `wmt14_fr_en_10k` dataset and load it into a HF `Dataset`. Split the data into a training and a validation set. To minimize the procedure, use the `.select(N)` method of the `Dataset` to use only 3000 samples for training and 50 for validation. Then, print the data, to see their feaures and ensure the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c9f56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cac6d27",
   "metadata": {},
   "source": [
    "**`TODO:`** Define a function that receives some samples and then uses the tokenizer we have defined to tokenize the samples. Append the phrase `\"translate English to French: \"` to the inputs. Tokenize the targets (french translation) as well. Use the `Dataset.map`method that we have previously discussed to apply your function to all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df33ebc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f2652e7",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "- Hugging Face's library for evaluations.\n",
    "- By importing `evaluate` the library provides ready-to-use implementations of common metrics (accuracy, F1, BLEU, ROUGE, etc.).\n",
    "- The `evaluate.load(\"sacrebleu\")` method, loads the SacreBLEU metric, a standard metric for evaluating machine translation quality. Give it a try in the following example. Any ideas about what the results could mean?\n",
    "\n",
    "More information regarding HF Evaluate can be found in the following [documentation](https://huggingface.co/docs/evaluate/en/index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68abbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "predictions = [\"the cat is on the mat\"]\n",
    "references = [[\"there is a cat on the mat\"]]\n",
    "\n",
    "results = sacrebleu.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d76432b",
   "metadata": {},
   "source": [
    "### Data COllators (continued)\n",
    "- The DataCollatorForSeq2Seq is a special collator for sequence-to-sequence tasks (like translation or summarization). It not only handles dynamic padding, but also makes sure that both the inputs and the labels (decoder side) are correctly padded. It can also prepare the labels for the model’s loss function (e.g. replacing padding tokens with -100 so they’re ignored during loss computation).\n",
    "- Because of this, the model as well as the tokenizer are required to initialize it: it uses the model’s configuration (e.g. label_pad_token_id, eos_token_id) to ensure that the labels it produces match exactly what the model expects for training and loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95805e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac159a76",
   "metadata": {},
   "source": [
    "Same as before, we need to be able to evaluate our model. Below is the evaluation method that we have chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f3e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Convert token IDs back to text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in labels as the padding token ID, then decode\n",
    "    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # SacreBLEU expects a list of prediction strings, plus a list of lists for references\n",
    "    results = sacrebleu.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=[[lbl] for lbl in decoded_labels]\n",
    "    )\n",
    "    return {\"bleu\": results[\"score\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797cbc87",
   "metadata": {},
   "source": [
    "**`TODO:`** In the same way as previously, define the `Seq2SeqTrainingArguments`, the `Seq2SeqTrainer` and train the model. Once it's done evaluate the final model.\n",
    "\n",
    "Notes:\n",
    "- You now have a validation split as well which the trainer can use.\n",
    "- The evaluation method given to you `compute_metrics` can also be given to the `Seq2SeqTrainer` for the same reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91611351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
