{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ba72b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pip3 install torch torchvision torchaudio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad20272",
   "metadata": {},
   "source": [
    "## 1. Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad34ffc",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "\n",
    "Activation functions are crucial components of neural networks because they introduce non-linear behavior. Without them, a network composed of stacked linear transformations would still behave like a single linear model, limiting its ability to capture complex patterns in the data.\n",
    "\n",
    "One of the most widely used activation functions today is the Rectified Linear Unit (ReLU), defined as\n",
    "\n",
    "\\begin{align}\n",
    "ReLU(x) = \\begin{cases}\n",
    "               0               & x<0\\\\\n",
    "               x               & x\\geq 0\\\\ \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**1.1 Find the derivative of the ReLU function, $R'(x)$. Carefully consider the two regions when $x<0$ and $x\\geq0$.** <br>\n",
    "\n",
    "---\n",
    "\n",
    "*Your answer here:*  \n",
    "\n",
    "When x < 0: 0\n",
    "Otherwise: 1\n",
    "\n",
    "---\n",
    "**1.2 We now want to implement the ReLU activation in code. Fill in the missing parts of the following class so that it computes both the activation and its derivative.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa998fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        return x if not (x < 0) else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        return not (x < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fbdf442",
   "metadata": {},
   "outputs": [],
   "source": [
    "myReLU = ReLU()\n",
    "assert(myReLU.forward(5) == 5)\n",
    "assert(myReLU.forward(0) == 0)\n",
    "assert(myReLU.forward(-1) == 0)\n",
    "\n",
    "assert(myReLU.gradient(5) == 1)\n",
    "assert(myReLU.gradient(0) == 1)\n",
    "assert(myReLU.gradient(-1) == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a1574",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "\n",
    "One of the classical choices is the sigmoid function, which smoothly squashes any real-valued input into the range (0,1). This property made it popular in the early days of neural networks, particularly for binary classification problems, since its output can be interpreted as a probability.\n",
    "\n",
    "\\begin{align}\n",
    "S(z) = \\frac{1}{1 + e^{-z}}.\n",
    "\\end{align}\n",
    "\n",
    "**1.3 Find the derivative of the Sigmoid function, $S'(x)$.** <br>\n",
    "---\n",
    "\n",
    "*Your answer here:*  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479376f1",
   "metadata": {},
   "source": [
    "By reciprocal rule\n",
    "\\begin{align}\n",
    "S'(x) &= \\frac{e^{-x}}{(1 + e^{-x})(1 + e^{-x})}\\\\\n",
    "      &= \\frac{e^{-x}}{(1 + e^{-x} + e^{-2x})}\\\\\n",
    "      ...\\\\\n",
    "      &=\\frac{1}{1+e^{-x}}(1-\\frac{1}{1+e^{-x}})\n",
    "\\end{align}\n",
    "---\n",
    "**1.4 We now want to implement the Sigmoid activation in code. Fill in the missing parts of the following class so that it computes both the activation and its derivative.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3372e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#S(z) = \\frac{1}{1 + e^{-z}}.\n",
    "class Sigmoid:\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        return 1/(1+math.exp(-x))\n",
    "\n",
    "    # S(x)(1−S(x))\n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        s = Sigmoid.forward(x)\n",
    "        return s * (1 - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c725c4da",
   "metadata": {},
   "source": [
    "**1.5 For both activations compute both their output and derivative values for the following range.**\n",
    "- Hint: Use a **2 × 2 grid of subplots**.\n",
    "- Hint: Label your axes, add **titles** for each subplot, and don’t forget **legends**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bcbc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-10, 10, 100)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(10, 8),sharey=True)\n",
    "\n",
    "# TODO: plot ReLU and Sigmoid functions and their derivatives\n",
    "mySigmoid = Sigmoid()\n",
    "sigmoidxs = [mySigmoid.forward(p) for p in x]\n",
    "sigmoidprimexs = [mySigmoid.gradient(p) for p in x]\n",
    "reluxs = [myReLU.forward(p) for p in x]\n",
    "reluprimexs = [myReLU.gradient(p) for p in x]\n",
    "\n",
    "ax1.plot(x, sigmoidxs)\n",
    "ax1.set_title('Sigmoid')\n",
    "ax1.legend('x')\n",
    "ax2.plot(x, sigmoidprimexs)\n",
    "ax2.set_title('Sigmoid\\'')\n",
    "ax3.plot(x, reluxs)\n",
    "ax3.set_title('ReLU')\n",
    "ax4.plot(x, reluprimexs)\n",
    "ax4.set_title('ReLU\\'')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e3f6b3",
   "metadata": {},
   "source": [
    "## 2. The Perceptron\n",
    "\n",
    "The **perceptron** is the simplest model of a neuron.  \n",
    "Given an input vector $(x \\in \\mathbb{R}^d)$, it computes:\n",
    "\n",
    "\\begin{align}\n",
    "y = \\phi\\!\\left(\\sum_{i=1}^{d} w_i x_i + b\\right),\n",
    "\\end{align}\n",
    "\n",
    "where  \n",
    "- $(x_i)$ are the inputs,  \n",
    "- $(w_i)$ are the corresponding weights,  \n",
    "- $(b)$ is the bias term,  \n",
    "- $(\\phi(\\cdot))$ is the activation function (e.g., sigmoid, ReLU, etc.).  \n",
    "\n",
    "**2.1 Implement a `Perceptron` class with:**\n",
    "   - a constructor that initializes weights and bias randomly,  \n",
    "   - a `forward(x)` method that returns the activated output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d52e3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # TODO: initialize weight matrix W and bias vector b\n",
    "        self.W = torch.rand(output_dim,input_dim)\n",
    "        self.b = torch.rand(output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: compute z = w·x + b\n",
    "        # TODO: apply sigmoid activation\n",
    "        z = self.W@x+self.b\n",
    "        return Sigmoid.forward(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46923257",
   "metadata": {},
   "source": [
    "**2.2 To verify your implementation recreate the Perceptron using Pytorch's built-in `torch.nn` modules and check that they behave in the same way**\n",
    "\n",
    "PyTorch already provides layers for you:\n",
    "\n",
    "- Look at `torch.nn.Linear` for the weight + bias computation.\n",
    "- Don’t forget to apply an activation function afterwards — check out modules in `torch.nn`.\n",
    "- You can combine them in sequence using `nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "331956ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference: tensor([0.4380], grad_fn=<SigmoidBackward0>) 0.43799208769800985\n"
     ]
    }
   ],
   "source": [
    "# Create both layers\n",
    "in_features, out_features = 3, 1\n",
    "my_layer =  nn.Linear(in_features,out_features)\n",
    "TorchPerceptron = nn.Sequential(\n",
    "    my_layer,\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Copy parameters from your layer to torch.nn.Linear\n",
    "with torch.no_grad():\n",
    "    TorchPerceptron[0].weight.copy_(my_layer.weight)\n",
    "    TorchPerceptron[0].bias.copy_(my_layer.bias)\n",
    "\n",
    "# Test input\n",
    "x = torch.rand(in_features)\n",
    "\n",
    "# Forward pass\n",
    "# TODO: compute outputs from both layers\n",
    "torchPerceptronResult = TorchPerceptron.forward(x)\n",
    "\n",
    "myPerceptron = Perceptron(3,1)\n",
    "\n",
    "# Copy parameters from torch.nn.Linear to your custom perceptron\n",
    "with torch.no_grad():\n",
    "    myPerceptron.W.copy_(my_layer.weight)\n",
    "    myPerceptron.b.copy_(my_layer.bias)\n",
    "    \n",
    "myPerceptronResult = myPerceptron.forward(x)\n",
    "\n",
    "print(\"Difference:\", torchPerceptronResult, myPerceptronResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab180d4f",
   "metadata": {},
   "source": [
    "## 3. The Linear Layer\n",
    "\n",
    "A perceptron takes one input vector and produces a single output after applying a weighted sum, a bias, and an activation.  \n",
    "\n",
    "A **linear layer** is simply a collection of multiple perceptrons stacked together.  \n",
    "- Instead of one weight vector $(w)$, we now have a weight matrix $(W \\in \\mathbb{R}^{m \\times d})$.  \n",
    "- Each row of $(W)$ corresponds to the weights of one perceptron.  \n",
    "- The bias term becomes a vector $(b \\in \\mathbb{R}^m)$.  \n",
    "- The output is a vector $(y \\in \\mathbb{R}^m)$:  \n",
    "\n",
    "\\begin{align}\n",
    "y &= W x + b\n",
    "\\end{align}\n",
    "\n",
    "where  \n",
    "- $x$ is the input of dimension $d$,  \n",
    "- $W$ applies $m$ linear combinations of the inputs,  \n",
    "- $b$ shifts (translates) the result.\n",
    "\n",
    "This is often called an **affine transformation**: a linear transformation plus a translation.\n",
    "\n",
    "**3.1 Fill the following code to implement your own Linear Layer class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9c926cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        # TODO: initialize weight matrix W and bias vector b\n",
    "        self.W = torch.rand(out_features,in_features)\n",
    "        self.b = torch.rand(out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: implement y = Wx + b\n",
    "        return self.W@x+self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cbb65e",
   "metadata": {},
   "source": [
    "**3.2 To verify your implementation recreate the Linear Layer using Pytorch's built-in `torch.nn` modules and check that they behave in the same way**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "32ca75a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference: tensor([0.0976, 0.9280, 1.3939]) tensor([0.0976, 0.9280, 1.3939], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create both layers\n",
    "in_features, out_features = 1, 3\n",
    "my_layer = Linear(in_features,out_features)\n",
    "TorchLinearLayer = torch.nn.Linear(in_features,out_features)\n",
    "\n",
    "# Copy parameters from your layer to torch.nn.Linear\n",
    "with torch.no_grad():\n",
    "    TorchLinearLayer.weight.copy_(my_layer.W)\n",
    "    TorchLinearLayer.bias.copy_(my_layer.b)\n",
    "\n",
    "# Test input\n",
    "x = torch.rand(in_features)\n",
    "\n",
    "# Forward pass\n",
    "# TODO: compute outputs from both layers\n",
    "a = my_layer.forward(x)\n",
    "b= TorchLinearLayer.forward(x)\n",
    "print(\"Difference:\", a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d47d42",
   "metadata": {},
   "source": [
    "## 4. Multi-Layer Perceptron (MLP)\n",
    "So far, you built:\n",
    "- a Perceptron → single linear unit + nonlinearity\n",
    "- a Linear layer → general affine transformation (x @ W^T + b)\n",
    "\n",
    "These are the *building blocks* of neural networks.\n",
    "\n",
    "A **Multi-Layer Perceptron (MLP)** is simply a stack of perceptrons (linear layers with nonlinearities between them).\n",
    "\n",
    "- The first layer transforms the input into a hidden representation.\n",
    "- A nonlinear activation (e.g., ReLU, Sigmoid, Tanh) makes the model expressive.\n",
    "- The next layer(s) take the hidden representation and produce outputs.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{a}_{l} &= {\\mathbf{W}}_{l}^T \\mathbf{x}_{l-1} + \\mathbf{b}_{l}\\;. \\\\\n",
    "\\end{align}\n",
    "\n",
    "Followed by the nonlinear activation \n",
    "\\begin{align}\n",
    "\\mathbf{x}_{l} &= f_{l}(\\mathbf{a}_{l})\n",
    "\\end{align}\n",
    "\n",
    "where:\n",
    "- $f_l$; Activation function for layer $l$\n",
    "- $x_l$: The output of layer $l$.\n",
    "- ${\\mathbf{W}}_{l}$ Weights of layer $l$.\n",
    "- $\\mathbf{b}_{l}$ Bias of layer $l$.\n",
    "\n",
    "**4.1 Fill the following code to implement your own MLP class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "84066596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        # TODO: initialize weight matrix W and bias vector b\n",
    "        self.W = torch.rand(out_features,in_features)\n",
    "        self.b = torch.rand(out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:  # Single example\n",
    "            return self.W @ x + self.b\n",
    "        else:  # Batch\n",
    "            return x @ self.W.T + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e8a29bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyMLP(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #TODO: implement the forward pass\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aacf53d",
   "metadata": {},
   "source": [
    "**4.2 To verify your implementation recreate the MLP using Pytorch's built-in `torch.nn` modules and check that they behave in the same way**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c126cc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single: True tensor([2.0912]) tensor([2.0912], grad_fn=<ViewBackward0>)\n",
      "batch: True tensor([[0.6816],\n",
      "        [0.6816],\n",
      "        [0.6816],\n",
      "        [1.5093],\n",
      "        [3.3096]]) tensor([[0.6816],\n",
      "        [0.6816],\n",
      "        [0.6816],\n",
      "        [1.5093],\n",
      "        [3.3096]], grad_fn=<AddmmBackward0>)\n",
      "All close: True\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "D_in, H, D_out = 3, 4, 1\n",
    "\n",
    "manual = MyMLP(D_in, H, D_out)\n",
    "TorchMLP = nn.Sequential(\n",
    "    nn.Linear(D_in, H),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H, D_out) \n",
    ")\n",
    "\n",
    "# TODO: Copy weights so forward passes should match exactly\n",
    "with torch.no_grad():\n",
    "    TorchMLP[0].weight.copy_(manual.network[0].W) \n",
    "    TorchMLP[0].bias.copy_(manual.network[0].b)\n",
    "    TorchMLP[2].weight.copy_(manual.network[2].W)\n",
    "    TorchMLP[2].bias.copy_(manual.network[2].b)\n",
    "\n",
    "# Single example\n",
    "x1 = torch.tensor([0.5, -1.0, 2.0])\n",
    "y_manual_1 = manual(x1)\n",
    "y_nn_1 = TorchMLP(x1)\n",
    "print(\"single:\", torch.allclose(y_manual_1, y_nn_1, atol=1e-7), y_manual_1, y_nn_1)\n",
    "\n",
    "# Test input\n",
    "xB = torch.randn(5, D_in)\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "# TODO: compute outputs from both layers\n",
    "y_manual_B = manual(xB)\n",
    "y_nn_B = TorchMLP(xB)\n",
    "print(\"batch:\", torch.allclose(y_manual_B, y_nn_B, atol=1e-7), y_manual_B, y_nn_B)\n",
    "\n",
    "print(\"All close:\", torch.allclose(y_manual_1, y_nn_1, atol=1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ee2bab",
   "metadata": {},
   "source": [
    "## 5. Mean Squared Error (MSE) Loss\n",
    "\n",
    "The Mean Squared Error (MSE) measures how far predictions are from the true values by averaging the squared differences.\n",
    "\n",
    "For $N$ predictions $\\hat{y}_i$ and true labels $y_i$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}_{\\text{MSE}} &= \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i)^2\n",
    "\\end{align}\n",
    "\n",
    "**5.1 Find the derivative of the MSE function** <br>\n",
    "\n",
    "---\n",
    "\n",
    "*Your answer here:*  \n",
    "y′=f′(g(x))⋅g′(x).\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}_{\\text{MSE}} prime &= 1/N(2*(\\hat{y}_i - y_i)*1)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "&=2/N(\\hat{y}_i - y_i)\n",
    "\\end{align}\n",
    "\n",
    "w.r.t. yhatj\n",
    "\n",
    "---\n",
    "**5.2 We now want to implement the MSE Loss in code. Fill in the missing parts of the following class so that it computes both the activation and its derivative.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "33006ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    @staticmethod\n",
    "    def loss(y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.mean((y_pred-y_true)**2)\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n",
    "        return 2/torch.numel(y_true)*(y_pred-y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a95beb5",
   "metadata": {},
   "source": [
    "**5.3 Compare against PyTorch’s built-in implementation. You can use `torch.nn.MSELoss` to compute the loss and gradients automatically.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "7185aaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch loss: 0.10000041872262955\n",
      "Manual loss: 0.10000041872262955\n",
      "\n",
      "PyTorch gradient:\n",
      " tensor([[ 0.0108, -0.0828, -0.0423],\n",
      "        [-0.0103,  0.0865,  0.0138],\n",
      "        [ 0.0220, -0.0245, -0.0014],\n",
      "        [ 0.0097, -0.0440,  0.0509],\n",
      "        [ 0.0658,  0.0019, -0.0089]])\n",
      "Manual gradient:\n",
      " tensor([[ 0.0108, -0.0828, -0.0423],\n",
      "        [-0.0103,  0.0865,  0.0138],\n",
      "        [ 0.0220, -0.0245, -0.0014],\n",
      "        [ 0.0097, -0.0440,  0.0509],\n",
      "        [ 0.0658,  0.0019, -0.0089]], grad_fn=<MulBackward0>)\n",
      "\n",
      "Loss close?   True\n",
      "Grad close?   True\n"
     ]
    }
   ],
   "source": [
    "N, C = 5, 3\n",
    "y_true = torch.rand(N, C)                      \n",
    "y_pred = torch.rand(N, C, requires_grad=True)  \n",
    "\n",
    "# --- Built-in PyTorch loss ---\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "torch_loss = loss_fn(y_pred, y_true)\n",
    "\n",
    "# Backprop to get gradients\n",
    "torch_loss.backward()\n",
    "torch_grad = y_pred.grad.clone()\n",
    "\n",
    "# --- Your implementation ---\n",
    "manual_loss = MSE.loss(y_true,y_pred)\n",
    "manual_grad = MSE.gradient(y_true,y_pred)\n",
    "\n",
    "# --- Compare ---\n",
    "print(\"PyTorch loss:\", torch_loss.item())\n",
    "print(\"Manual loss:\", manual_loss.item())\n",
    "\n",
    "print(\"\\nPyTorch gradient:\\n\", torch_grad)\n",
    "print(\"Manual gradient:\\n\", manual_grad)\n",
    "\n",
    "print(\"\\nLoss close?  \", torch.allclose(torch_loss, manual_loss))\n",
    "print(\"Grad close?  \", torch.allclose(torch_grad, manual_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8268c41",
   "metadata": {},
   "source": [
    "## 6. The whole Pipeline: Training on FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3a1225",
   "metadata": {},
   "source": [
    "### Loading the FashionMNIST Dataset\n",
    "\n",
    "We will use **FashionMNIST**, a dataset of grayscale 28×28 images of clothing items (e.g., shirts, shoes, bags).  \n",
    "It is built into PyTorch and can be easily downloaded.\n",
    "\n",
    "#### Datasets and Dataloaders\n",
    "- A **Dataset** object (like `datasets.FashionMNIST`) gives you access to the data samples and their labels.  \n",
    "- A **DataLoader** wraps a dataset and helps you:\n",
    "  - Load the data in **mini-batches** (instead of one sample at a time).  \n",
    "  - **Shuffle** the data during training (good for generalization).  \n",
    "  - Use multiple worker processes to speed up loading.  \n",
    "\n",
    "In practice, you almost always combine a Dataset with a DataLoader when training models in PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e1b43877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:01<00:00, 14.8MB/s]\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 452kB/s]\n",
      "100%|██████████| 4.42M/4.42M [00:00<00:00, 6.25MB/s]\n",
      "100%|██████████| 5.15k/5.15k [00:00<?, ?B/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# Setup the Dataloader for training\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "img, label = training_data[sample_idx]\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "e6e460c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set(training_data.classes)\n",
    "b = set(test_data.classes)\n",
    "\n",
    "len(a.union(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb04bd",
   "metadata": {},
   "source": [
    "**5.1 How many features does each sample have?**\n",
    "28*28 features due to it being grayscale.\n",
    "\n",
    "**5.2 How many classes do we have in the dataset?**\n",
    "10\n",
    "---\n",
    "\n",
    "*Your answers here:*  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0dc83d",
   "metadata": {},
   "source": [
    "### Choosing a Device (CPU or GPU)\n",
    "\n",
    "Training deep learning models can be much faster on a GPU, if one is available.  \n",
    "In PyTorch, we usually set a `device` variable so that both the model and the data can be placed consistently on either:\n",
    "\n",
    "- **GPU (`\"cuda\"`)** → preferred for faster training when available  \n",
    "- **CPU (`\"cpu\"`)** → always available, sufficient for small exercises  \n",
    "\n",
    "For this exercise, using a GPU is **not required** — but it’s good practice to write code that supports both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a6a885c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Select GPU if available, otherwise fall back to CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b6730d",
   "metadata": {},
   "source": [
    "### Defining a Neural Network in PyTorch\n",
    "\n",
    "We now define a simple **feedforward neural network** for image classification.\n",
    "\n",
    "- **`nn.Flatten()`** → converts a 2D image (28×28 pixels) into a 1D tensor (length 784).  \n",
    "- **`nn.Sequential()`** → a container that runs layers in order. Here it includes:  \n",
    "  1. A linear (fully connected) layer mapping from `28*28` inputs to a hidden dimension.  \n",
    "  2. A **ReLU** activation function for nonlinearity.  \n",
    "  3. Another linear layer mapping from the hidden dimension to 10 output classes.\n",
    "\n",
    "The network returns **logits** (unnormalized scores for each class).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8113338d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_features, out_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "# Initialize the model with the appropriate arguments based on our dataset and move it to the device.\n",
    "# Hint: For hidden features you can use 128\n",
    "model = NeuralNetwork(28*28,128,10)\n",
    "\n",
    "# Display the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbea533f",
   "metadata": {},
   "source": [
    "### Hyperparameters, Loss Function, and Optimizer\n",
    "\n",
    "To train our neural network, we need to set a few key choices:\n",
    "\n",
    "- **Learning rate**: controls how big each parameter update step is.  \n",
    "- **Batch size**: number of samples processed together before updating weights.  \n",
    "- **Epochs**: how many full passes we make over the training dataset.  \n",
    "\n",
    "We also need:\n",
    "\n",
    "- **Loss function**: measures how far the model’s predictions are from the true labels.  \n",
    "  - Here we use **Cross-Entropy Loss**, the standard choice for multi-class classification.  \n",
    "- **Optimizer**: updates model parameters using the gradients.  \n",
    "  - Here we use **Stochastic Gradient Descent (SGD)** with the chosen learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "6d0b57d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# Loss function (for classification)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (SGD with given learning rate)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e95d3b5",
   "metadata": {},
   "source": [
    "### Training and Evaluating a Neural Network\n",
    "\n",
    "Once we have a dataset and a model, the next step is to **train** the model so that it can make accurate predictions.\n",
    "\n",
    "### Training\n",
    "Training is the process of teaching the model to minimize a **loss function** by adjusting its parameters (weights and biases).  \n",
    "This is done using an algorithm called **backpropagation** combined with an **optimizer** (such as SGD or Adam).\n",
    "We will be looking closer at backpropagation in the next sessions.\n",
    "\n",
    "The training loop typically consists of:\n",
    "1. **Forward pass** → feed the input through the model to get predictions.  \n",
    "2. **Compute the loss** → measure how far predictions are from the true labels.  \n",
    "3. **Backward pass** → compute gradients of the loss with respect to model parameters.  \n",
    "4. **Update parameters** → use the optimizer to adjust weights and biases.  \n",
    "\n",
    "Repeating this process over the dataset (for multiple **epochs**) gradually improves the model.\n",
    "\n",
    "### Evaluation\n",
    "After training, we need to evaluate the model on **unseen data** (the test set).  \n",
    "During evaluation:\n",
    "- We disable gradient calculations (`torch.no_grad()`), since we are not training.  \n",
    "- The model is set to **evaluation mode** (`model.eval()`), which is important for certain layers (e.g., dropout, batch normalization).  \n",
    "- We measure **accuracy** and **average loss** to understand how well the model generalizes.\n",
    "\n",
    "\n",
    "**5.1 Complete the training loop and train the model for 10 epochs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "2f6a2889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.318361\n",
      "loss: 2.274389\n",
      "loss: 2.219187\n",
      "loss: 2.200776\n",
      "loss: 2.131717\n",
      "loss: 2.074260\n",
      "loss: 2.065700\n",
      "loss: 1.988261\n",
      "loss: 2.006869\n",
      "loss: 1.893124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3183605670928955,\n",
       " 2.3075973987579346,\n",
       " 2.3117897510528564,\n",
       " 2.3179235458374023,\n",
       " 2.326648235321045,\n",
       " 2.31795072555542,\n",
       " 2.3285367488861084,\n",
       " 2.336446523666382,\n",
       " 2.3089475631713867,\n",
       " 2.338036060333252,\n",
       " 2.3263041973114014,\n",
       " 2.3138909339904785,\n",
       " 2.2832531929016113,\n",
       " 2.304072141647339,\n",
       " 2.2984511852264404,\n",
       " 2.320502996444702,\n",
       " 2.3166720867156982,\n",
       " 2.322389841079712,\n",
       " 2.3115761280059814,\n",
       " 2.3217453956604004,\n",
       " 2.317575454711914,\n",
       " 2.3158488273620605,\n",
       " 2.3208088874816895,\n",
       " 2.2858188152313232,\n",
       " 2.302104949951172,\n",
       " 2.3098955154418945,\n",
       " 2.3048348426818848,\n",
       " 2.3079001903533936,\n",
       " 2.3003530502319336,\n",
       " 2.289010524749756,\n",
       " 2.310901165008545,\n",
       " 2.3163070678710938,\n",
       " 2.3063628673553467,\n",
       " 2.3005921840667725,\n",
       " 2.2988030910491943,\n",
       " 2.2981765270233154,\n",
       " 2.2941818237304688,\n",
       " 2.2883288860321045,\n",
       " 2.3083200454711914,\n",
       " 2.305405616760254,\n",
       " 2.3129489421844482,\n",
       " 2.289982557296753,\n",
       " 2.29203724861145,\n",
       " 2.2977700233459473,\n",
       " 2.318532705307007,\n",
       " 2.3023271560668945,\n",
       " 2.287109375,\n",
       " 2.286998987197876,\n",
       " 2.2810118198394775,\n",
       " 2.3127286434173584,\n",
       " 2.2867484092712402,\n",
       " 2.2862789630889893,\n",
       " 2.2962849140167236,\n",
       " 2.279736042022705,\n",
       " 2.293442964553833,\n",
       " 2.2876479625701904,\n",
       " 2.2949235439300537,\n",
       " 2.2906289100646973,\n",
       " 2.290316104888916,\n",
       " 2.2996909618377686,\n",
       " 2.288071870803833,\n",
       " 2.2717387676239014,\n",
       " 2.2806577682495117,\n",
       " 2.276068925857544,\n",
       " 2.2737247943878174,\n",
       " 2.2964773178100586,\n",
       " 2.27724289894104,\n",
       " 2.2726035118103027,\n",
       " 2.276601552963257,\n",
       " 2.2865254878997803,\n",
       " 2.293130874633789,\n",
       " 2.283809185028076,\n",
       " 2.2709574699401855,\n",
       " 2.278614044189453,\n",
       " 2.280385732650757,\n",
       " 2.2743945121765137,\n",
       " 2.294161081314087,\n",
       " 2.2570269107818604,\n",
       " 2.3003182411193848,\n",
       " 2.259228467941284,\n",
       " 2.2812368869781494,\n",
       " 2.2775917053222656,\n",
       " 2.287832736968994,\n",
       " 2.2766785621643066,\n",
       " 2.2704508304595947,\n",
       " 2.2731144428253174,\n",
       " 2.275357961654663,\n",
       " 2.2832143306732178,\n",
       " 2.2760114669799805,\n",
       " 2.2779808044433594,\n",
       " 2.272925615310669,\n",
       " 2.2759695053100586,\n",
       " 2.271947145462036,\n",
       " 2.2546708583831787,\n",
       " 2.2620041370391846,\n",
       " 2.2781691551208496,\n",
       " 2.2353463172912598,\n",
       " 2.2714431285858154,\n",
       " 2.2789466381073,\n",
       " 2.2760090827941895,\n",
       " 2.2743890285491943,\n",
       " 2.260298728942871,\n",
       " 2.27276873588562,\n",
       " 2.249009132385254,\n",
       " 2.2640233039855957,\n",
       " 2.250192880630493,\n",
       " 2.23881196975708,\n",
       " 2.258028984069824,\n",
       " 2.2552106380462646,\n",
       " 2.2627179622650146,\n",
       " 2.258685350418091,\n",
       " 2.2549731731414795,\n",
       " 2.2643661499023438,\n",
       " 2.2673089504241943,\n",
       " 2.2681124210357666,\n",
       " 2.2621724605560303,\n",
       " 2.258635997772217,\n",
       " 2.2550771236419678,\n",
       " 2.2535760402679443,\n",
       " 2.2562015056610107,\n",
       " 2.2579309940338135,\n",
       " 2.2834882736206055,\n",
       " 2.238969087600708,\n",
       " 2.263456106185913,\n",
       " 2.255610466003418,\n",
       " 2.2724673748016357,\n",
       " 2.245396375656128,\n",
       " 2.2538809776306152,\n",
       " 2.249457836151123,\n",
       " 2.254727363586426,\n",
       " 2.255610942840576,\n",
       " 2.2456772327423096,\n",
       " 2.2510571479797363,\n",
       " 2.252025842666626,\n",
       " 2.242600917816162,\n",
       " 2.2500970363616943,\n",
       " 2.2512176036834717,\n",
       " 2.2547218799591064,\n",
       " 2.264404535293579,\n",
       " 2.24894380569458,\n",
       " 2.2379884719848633,\n",
       " 2.240835189819336,\n",
       " 2.2450335025787354,\n",
       " 2.2427902221679688,\n",
       " 2.2378756999969482,\n",
       " 2.2359602451324463,\n",
       " 2.2456421852111816,\n",
       " 2.266883611679077,\n",
       " 2.23406982421875,\n",
       " 2.2492315769195557,\n",
       " 2.263383388519287,\n",
       " 2.2295947074890137,\n",
       " 2.247737169265747,\n",
       " 2.245600461959839,\n",
       " 2.233618974685669,\n",
       " 2.252687692642212,\n",
       " 2.2466518878936768,\n",
       " 2.2402117252349854,\n",
       " 2.2338781356811523,\n",
       " 2.249997615814209,\n",
       " 2.247860908508301,\n",
       " 2.219405174255371,\n",
       " 2.229727268218994,\n",
       " 2.2351937294006348,\n",
       " 2.247487783432007,\n",
       " 2.2430741786956787,\n",
       " 2.2371816635131836,\n",
       " 2.227367877960205,\n",
       " 2.245515823364258,\n",
       " 2.2333807945251465,\n",
       " 2.2485992908477783,\n",
       " 2.231290340423584,\n",
       " 2.2104721069335938,\n",
       " 2.231987476348877,\n",
       " 2.225515842437744,\n",
       " 2.2390074729919434,\n",
       " 2.2217743396759033,\n",
       " 2.2214300632476807,\n",
       " 2.2349860668182373,\n",
       " 2.2332639694213867,\n",
       " 2.243259906768799,\n",
       " 2.2037813663482666,\n",
       " 2.2355642318725586,\n",
       " 2.1982815265655518,\n",
       " 2.2347471714019775,\n",
       " 2.2316818237304688,\n",
       " 2.217930793762207,\n",
       " 2.2282724380493164,\n",
       " 2.214543581008911,\n",
       " 2.203070878982544,\n",
       " 2.217461585998535,\n",
       " 2.2297966480255127,\n",
       " 2.207498550415039,\n",
       " 2.2158570289611816,\n",
       " 2.2182865142822266,\n",
       " 2.216972827911377,\n",
       " 2.2125306129455566,\n",
       " 2.2116312980651855,\n",
       " 2.2152063846588135,\n",
       " 2.2212865352630615,\n",
       " 2.219186544418335,\n",
       " 2.2196056842803955,\n",
       " 2.231529712677002,\n",
       " 2.228487968444824,\n",
       " 2.1761040687561035,\n",
       " 2.2173852920532227,\n",
       " 2.213733196258545,\n",
       " 2.218270778656006,\n",
       " 2.2298314571380615,\n",
       " 2.235553741455078,\n",
       " 2.2208774089813232,\n",
       " 2.212075710296631,\n",
       " 2.2102487087249756,\n",
       " 2.1915180683135986,\n",
       " 2.2274162769317627,\n",
       " 2.2113890647888184,\n",
       " 2.20696759223938,\n",
       " 2.185253143310547,\n",
       " 2.229296922683716,\n",
       " 2.21806001663208,\n",
       " 2.2175686359405518,\n",
       " 2.2080843448638916,\n",
       " 2.2059829235076904,\n",
       " 2.2162160873413086,\n",
       " 2.197366237640381,\n",
       " 2.191103935241699,\n",
       " 2.1938364505767822,\n",
       " 2.2041871547698975,\n",
       " 2.188218116760254,\n",
       " 2.2148730754852295,\n",
       " 2.1903128623962402,\n",
       " 2.1962380409240723,\n",
       " 2.200422763824463,\n",
       " 2.2154722213745117,\n",
       " 2.1938016414642334,\n",
       " 2.196000337600708,\n",
       " 2.224419116973877,\n",
       " 2.2066986560821533,\n",
       " 2.2030553817749023,\n",
       " 2.22727632522583,\n",
       " 2.2072949409484863,\n",
       " 2.2036664485931396,\n",
       " 2.2055139541625977,\n",
       " 2.2166197299957275,\n",
       " 2.190871477127075,\n",
       " 2.1884536743164062,\n",
       " 2.2168304920196533,\n",
       " 2.198105573654175,\n",
       " 2.2194981575012207,\n",
       " 2.2173426151275635,\n",
       " 2.2279770374298096,\n",
       " 2.203619956970215,\n",
       " 2.1912686824798584,\n",
       " 2.197465658187866,\n",
       " 2.2037510871887207,\n",
       " 2.1887550354003906,\n",
       " 2.213698625564575,\n",
       " 2.1700501441955566,\n",
       " 2.173753499984741,\n",
       " 2.179553508758545,\n",
       " 2.1861510276794434,\n",
       " 2.1857283115386963,\n",
       " 2.1709675788879395,\n",
       " 2.187595844268799,\n",
       " 2.1823692321777344,\n",
       " 2.2055609226226807,\n",
       " 2.1938018798828125,\n",
       " 2.194753885269165,\n",
       " 2.1935806274414062,\n",
       " 2.203249216079712,\n",
       " 2.176055431365967,\n",
       " 2.2164955139160156,\n",
       " 2.1793577671051025,\n",
       " 2.199031352996826,\n",
       " 2.1837375164031982,\n",
       " 2.201417922973633,\n",
       " 2.1807518005371094,\n",
       " 2.192570447921753,\n",
       " 2.188938617706299,\n",
       " 2.1934328079223633,\n",
       " 2.1959028244018555,\n",
       " 2.183206796646118,\n",
       " 2.1850578784942627,\n",
       " 2.1994752883911133,\n",
       " 2.190479278564453,\n",
       " 2.202327013015747,\n",
       " 2.1950042247772217,\n",
       " 2.152033567428589,\n",
       " 2.1892600059509277,\n",
       " 2.1859898567199707,\n",
       " 2.1885933876037598,\n",
       " 2.19106388092041,\n",
       " 2.1870296001434326,\n",
       " 2.1842565536499023,\n",
       " 2.196211814880371,\n",
       " 2.1914172172546387,\n",
       " 2.191638946533203,\n",
       " 2.152026891708374,\n",
       " 2.16614031791687,\n",
       " 2.167884349822998,\n",
       " 2.200775623321533,\n",
       " 2.1804068088531494,\n",
       " 2.16031551361084,\n",
       " 2.149898052215576,\n",
       " 2.1791915893554688,\n",
       " 2.1659393310546875,\n",
       " 2.186448574066162,\n",
       " 2.1494555473327637,\n",
       " 2.1703691482543945,\n",
       " 2.1785104274749756,\n",
       " 2.1790475845336914,\n",
       " 2.2060015201568604,\n",
       " 2.1590092182159424,\n",
       " 2.1838881969451904,\n",
       " 2.168710947036743,\n",
       " 2.2048165798187256,\n",
       " 2.1747539043426514,\n",
       " 2.171689510345459,\n",
       " 2.1791555881500244,\n",
       " 2.1794164180755615,\n",
       " 2.1499087810516357,\n",
       " 2.1764349937438965,\n",
       " 2.1974215507507324,\n",
       " 2.145399332046509,\n",
       " 2.170581340789795,\n",
       " 2.1662278175354004,\n",
       " 2.1548118591308594,\n",
       " 2.1832144260406494,\n",
       " 2.1732776165008545,\n",
       " 2.157107353210449,\n",
       " 2.146690607070923,\n",
       " 2.147841691970825,\n",
       " 2.1854565143585205,\n",
       " 2.1612515449523926,\n",
       " 2.1605618000030518,\n",
       " 2.169139862060547,\n",
       " 2.1459152698516846,\n",
       " 2.1383469104766846,\n",
       " 2.1736884117126465,\n",
       " 2.157599925994873,\n",
       " 2.174193859100342,\n",
       " 2.1756644248962402,\n",
       " 2.15836501121521,\n",
       " 2.1778926849365234,\n",
       " 2.136733293533325,\n",
       " 2.1761770248413086,\n",
       " 2.136176109313965,\n",
       " 2.139847993850708,\n",
       " 2.1744184494018555,\n",
       " 2.164966583251953,\n",
       " 2.1503875255584717,\n",
       " 2.1824800968170166,\n",
       " 2.164581775665283,\n",
       " 2.191884994506836,\n",
       " 2.1328747272491455,\n",
       " 2.123366594314575,\n",
       " 2.1690783500671387,\n",
       " 2.1498448848724365,\n",
       " 2.1480674743652344,\n",
       " 2.1630232334136963,\n",
       " 2.156848669052124,\n",
       " 2.187592029571533,\n",
       " 2.155646324157715,\n",
       " 2.157557725906372,\n",
       " 2.1664786338806152,\n",
       " 2.171251058578491,\n",
       " 2.1553995609283447,\n",
       " 2.139425754547119,\n",
       " 2.145303726196289,\n",
       " 2.1641745567321777,\n",
       " 2.1639883518218994,\n",
       " 2.1448686122894287,\n",
       " 2.128030300140381,\n",
       " 2.152036190032959,\n",
       " 2.170166015625,\n",
       " 2.1425490379333496,\n",
       " 2.183380365371704,\n",
       " 2.126976490020752,\n",
       " 2.1496775150299072,\n",
       " 2.1530280113220215,\n",
       " 2.1643638610839844,\n",
       " 2.1239583492279053,\n",
       " 2.1280901432037354,\n",
       " 2.1323964595794678,\n",
       " 2.1161305904388428,\n",
       " 2.1445116996765137,\n",
       " 2.1481480598449707,\n",
       " 2.1307473182678223,\n",
       " 2.1399669647216797,\n",
       " 2.1340489387512207,\n",
       " 2.134652614593506,\n",
       " 2.150810956954956,\n",
       " 2.1655828952789307,\n",
       " 2.1445467472076416,\n",
       " 2.113597869873047,\n",
       " 2.112013339996338,\n",
       " 2.177954912185669,\n",
       " 2.1294963359832764,\n",
       " 2.104426145553589,\n",
       " 2.1512229442596436,\n",
       " 2.1317172050476074,\n",
       " 2.1224586963653564,\n",
       " 2.1379458904266357,\n",
       " 2.1322779655456543,\n",
       " 2.1298184394836426,\n",
       " 2.1350975036621094,\n",
       " 2.1255884170532227,\n",
       " 2.135453939437866,\n",
       " 2.1406333446502686,\n",
       " 2.1477184295654297,\n",
       " 2.1234686374664307,\n",
       " 2.1269140243530273,\n",
       " 2.191166877746582,\n",
       " 2.1390748023986816,\n",
       " 2.149890184402466,\n",
       " 2.1295666694641113,\n",
       " 2.1331515312194824,\n",
       " 2.144016981124878,\n",
       " 2.111785411834717,\n",
       " 2.1078436374664307,\n",
       " 2.092024087905884,\n",
       " 2.1368424892425537,\n",
       " 2.1143643856048584,\n",
       " 2.1064040660858154,\n",
       " 2.1491527557373047,\n",
       " 2.124936819076538,\n",
       " 2.1338956356048584,\n",
       " 2.1425106525421143,\n",
       " 2.130995512008667,\n",
       " 2.138928174972534,\n",
       " 2.141045570373535,\n",
       " 2.139950752258301,\n",
       " 2.125216484069824,\n",
       " 2.1332733631134033,\n",
       " 2.1109557151794434,\n",
       " 2.139509439468384,\n",
       " 2.1716294288635254,\n",
       " 2.136275053024292,\n",
       " 2.1112542152404785,\n",
       " 2.1532399654388428,\n",
       " 2.10526704788208,\n",
       " 2.121511459350586,\n",
       " 2.113530158996582,\n",
       " 2.108426570892334,\n",
       " 2.1141061782836914,\n",
       " 2.1485555171966553,\n",
       " 2.0998458862304688,\n",
       " 2.1527223587036133,\n",
       " 2.105027198791504,\n",
       " 2.1350345611572266,\n",
       " 2.103754758834839,\n",
       " 2.103790760040283,\n",
       " 2.1031885147094727,\n",
       " 2.109358787536621,\n",
       " 2.115825653076172,\n",
       " 2.1156833171844482,\n",
       " 2.0948684215545654,\n",
       " 2.1210741996765137,\n",
       " 2.088719606399536,\n",
       " 2.106665849685669,\n",
       " 2.133683681488037,\n",
       " 2.139406681060791,\n",
       " 2.1316120624542236,\n",
       " 2.1337008476257324,\n",
       " 2.1171109676361084,\n",
       " 2.1033570766448975,\n",
       " 2.131092071533203,\n",
       " 2.0914745330810547,\n",
       " 2.106679677963257,\n",
       " 2.1264591217041016,\n",
       " 2.0864105224609375,\n",
       " 2.066652297973633,\n",
       " 2.124295949935913,\n",
       " 2.099245071411133,\n",
       " 2.0657272338867188,\n",
       " 2.1289587020874023,\n",
       " 2.11346697807312,\n",
       " 2.1132960319519043,\n",
       " 2.133638381958008,\n",
       " 2.101823091506958,\n",
       " 2.126896858215332,\n",
       " 2.093064308166504,\n",
       " 2.151301383972168,\n",
       " 2.131743907928467,\n",
       " 2.1086790561676025,\n",
       " 2.1159374713897705,\n",
       " 2.092535972595215,\n",
       " 2.0924251079559326,\n",
       " 2.121309995651245,\n",
       " 2.1057827472686768,\n",
       " 2.1156623363494873,\n",
       " 2.0848329067230225,\n",
       " 2.0919504165649414,\n",
       " 2.0939595699310303,\n",
       " 2.096773147583008,\n",
       " 2.0893568992614746,\n",
       " 2.076282501220703,\n",
       " 2.0886101722717285,\n",
       " 2.1135976314544678,\n",
       " 2.1046581268310547,\n",
       " 2.0742597579956055,\n",
       " 2.099248170852661,\n",
       " 2.077444314956665,\n",
       " 2.112299680709839,\n",
       " 2.0821046829223633,\n",
       " 2.1162283420562744,\n",
       " 2.0943870544433594,\n",
       " 2.093186616897583,\n",
       " 2.097684144973755,\n",
       " 2.0934062004089355,\n",
       " 2.0374631881713867,\n",
       " 2.107490062713623,\n",
       " 2.0974836349487305,\n",
       " 2.0922563076019287,\n",
       " 2.0719311237335205,\n",
       " 2.1051039695739746,\n",
       " 2.074669599533081,\n",
       " 2.0653815269470215,\n",
       " 2.0613255500793457,\n",
       " 2.1066408157348633,\n",
       " 2.0883891582489014,\n",
       " 2.0430352687835693,\n",
       " 2.0770175457000732,\n",
       " 2.054919958114624,\n",
       " 2.100954294204712,\n",
       " 2.122164249420166,\n",
       " 2.0687716007232666,\n",
       " 2.1136670112609863,\n",
       " 2.099414348602295,\n",
       " 2.0980212688446045,\n",
       " 2.095158815383911,\n",
       " 2.0590660572052,\n",
       " 2.0822677612304688,\n",
       " 2.065026044845581,\n",
       " 2.07505464553833,\n",
       " 2.069972276687622,\n",
       " 2.05873441696167,\n",
       " 2.1047720909118652,\n",
       " 2.0883164405822754,\n",
       " 2.0322258472442627,\n",
       " 2.0875587463378906,\n",
       " 2.0786311626434326,\n",
       " 2.114455461502075,\n",
       " 2.0489702224731445,\n",
       " 2.0884487628936768,\n",
       " 2.0163068771362305,\n",
       " 2.096714496612549,\n",
       " 2.052837371826172,\n",
       " 2.02329421043396,\n",
       " 2.0704989433288574,\n",
       " 2.0215015411376953,\n",
       " 2.070394515991211,\n",
       " 2.093576192855835,\n",
       " 2.0494654178619385,\n",
       " 2.0986905097961426,\n",
       " 2.0430264472961426,\n",
       " 2.016282320022583,\n",
       " 2.0893378257751465,\n",
       " 2.046997547149658,\n",
       " 2.0511081218719482,\n",
       " 2.047372579574585,\n",
       " 2.062391996383667,\n",
       " 2.0391812324523926,\n",
       " 2.018995523452759,\n",
       " 2.026763677597046,\n",
       " 2.10628080368042,\n",
       " 2.0635781288146973,\n",
       " 2.0914719104766846,\n",
       " 2.0522444248199463,\n",
       " 2.0344998836517334,\n",
       " 2.035651206970215,\n",
       " 2.0543484687805176,\n",
       " 2.0831053256988525,\n",
       " 2.0635805130004883,\n",
       " 2.106337308883667,\n",
       " 2.0554604530334473,\n",
       " 2.032741069793701,\n",
       " 2.0793557167053223,\n",
       " 2.0554139614105225,\n",
       " 2.076566219329834,\n",
       " 2.089073419570923,\n",
       " 2.031705379486084,\n",
       " 2.061677932739258,\n",
       " 2.0410284996032715,\n",
       " 2.0532546043395996,\n",
       " 2.0711240768432617,\n",
       " 2.063295364379883,\n",
       " 2.111346483230591,\n",
       " 2.0496349334716797,\n",
       " 2.0760879516601562,\n",
       " 2.076716661453247,\n",
       " 2.0604677200317383,\n",
       " 2.0876402854919434,\n",
       " 2.0218777656555176,\n",
       " 2.0714662075042725,\n",
       " 2.055849313735962,\n",
       " 2.05560564994812,\n",
       " 2.0640766620635986,\n",
       " 2.022528648376465,\n",
       " 2.081796884536743,\n",
       " 2.065699577331543,\n",
       " 2.0372095108032227,\n",
       " 2.0743536949157715,\n",
       " 2.0676112174987793,\n",
       " 2.0244805812835693,\n",
       " 2.08195161819458,\n",
       " 2.0491487979888916,\n",
       " 2.066451072692871,\n",
       " 2.015350341796875,\n",
       " 2.036205530166626,\n",
       " 2.033705234527588,\n",
       " 2.0586209297180176,\n",
       " 2.0652520656585693,\n",
       " 2.0302536487579346,\n",
       " 2.0259554386138916,\n",
       " 2.024380922317505,\n",
       " 2.0709147453308105,\n",
       " 2.052915573120117,\n",
       " 2.0744166374206543,\n",
       " 2.060953140258789,\n",
       " 2.049010992050171,\n",
       " 2.0260725021362305,\n",
       " 2.085643768310547,\n",
       " 2.0642855167388916,\n",
       " 2.031982421875,\n",
       " 2.048304796218872,\n",
       " 2.0227999687194824,\n",
       " 2.0447044372558594,\n",
       " 2.005307674407959,\n",
       " 2.028578281402588,\n",
       " 2.0197641849517822,\n",
       " 2.028071165084839,\n",
       " 2.009883403778076,\n",
       " 2.0275423526763916,\n",
       " 2.0680716037750244,\n",
       " 2.066622018814087,\n",
       " 2.077151298522949,\n",
       " 2.0483756065368652,\n",
       " 1.9970817565917969,\n",
       " 2.0523927211761475,\n",
       " 2.051074504852295,\n",
       " 2.0234086513519287,\n",
       " 2.0113630294799805,\n",
       " 1.982427716255188,\n",
       " 2.030646800994873,\n",
       " 2.081646203994751,\n",
       " 2.0296082496643066,\n",
       " 2.0007545948028564,\n",
       " 2.0452194213867188,\n",
       " 2.0557241439819336,\n",
       " 2.0187087059020996,\n",
       " 2.023345470428467,\n",
       " 2.023322343826294,\n",
       " 2.0568227767944336,\n",
       " 2.027503490447998,\n",
       " 2.0393693447113037,\n",
       " 2.01835298538208,\n",
       " 2.03487229347229,\n",
       " 2.029834747314453,\n",
       " 2.040717840194702,\n",
       " 2.0186686515808105,\n",
       " 1.994946002960205,\n",
       " 1.9878695011138916,\n",
       " 2.0096688270568848,\n",
       " 2.0191686153411865,\n",
       " 1.9653701782226562,\n",
       " 2.028952121734619,\n",
       " 2.0364608764648438,\n",
       " 2.032148599624634,\n",
       " 2.0294535160064697,\n",
       " 2.0509252548217773,\n",
       " 2.051273822784424,\n",
       " 2.005035161972046,\n",
       " 2.019181251525879,\n",
       " 2.025031328201294,\n",
       " 2.0252721309661865,\n",
       " 2.044635057449341,\n",
       " 2.0182406902313232,\n",
       " 2.0319433212280273,\n",
       " 1.9870301485061646,\n",
       " 2.007899522781372,\n",
       " 2.004645586013794,\n",
       " 1.963067889213562,\n",
       " 2.0082662105560303,\n",
       " 1.973879337310791,\n",
       " 2.050813674926758,\n",
       " 1.993363857269287,\n",
       " 1.9705113172531128,\n",
       " 2.0263304710388184,\n",
       " 1.9907786846160889,\n",
       " 1.9832059144973755,\n",
       " 2.0522348880767822,\n",
       " 1.985368013381958,\n",
       " 2.0037686824798584,\n",
       " 2.0273938179016113,\n",
       " 1.998945951461792,\n",
       " 1.9890425205230713,\n",
       " 1.9657713174819946,\n",
       " 2.004713535308838,\n",
       " 2.009855270385742,\n",
       " 1.9882606267929077,\n",
       " 2.015491247177124,\n",
       " 2.0299787521362305,\n",
       " 1.991231083869934,\n",
       " 2.028533935546875,\n",
       " 2.0491015911102295,\n",
       " 2.0506467819213867,\n",
       " 2.0067527294158936,\n",
       " 1.9961378574371338,\n",
       " 1.996232271194458,\n",
       " 2.0218894481658936,\n",
       " 1.943699836730957,\n",
       " 2.0316975116729736,\n",
       " 2.0026144981384277,\n",
       " 1.974245548248291,\n",
       " 2.000882148742676,\n",
       " 1.993700623512268,\n",
       " 2.0028328895568848,\n",
       " 1.9785314798355103,\n",
       " 1.9920400381088257,\n",
       " 2.030134439468384,\n",
       " 1.9494818449020386,\n",
       " 1.9717507362365723,\n",
       " 2.007105588912964,\n",
       " 1.992194652557373,\n",
       " 1.9793342351913452,\n",
       " 1.9810092449188232,\n",
       " 1.9927788972854614,\n",
       " 1.966488003730774,\n",
       " 1.9722843170166016,\n",
       " 2.082479238510132,\n",
       " 1.9993478059768677,\n",
       " 2.0361859798431396,\n",
       " 1.9929206371307373,\n",
       " 2.003614902496338,\n",
       " 2.0368876457214355,\n",
       " 1.98944890499115,\n",
       " 2.0067617893218994,\n",
       " 2.0062801837921143,\n",
       " 1.9794120788574219,\n",
       " 1.9934542179107666,\n",
       " 1.9990886449813843,\n",
       " 1.996541142463684,\n",
       " 2.0231130123138428,\n",
       " 1.9910577535629272,\n",
       " 1.9285500049591064,\n",
       " 1.9953337907791138,\n",
       " 2.001755714416504,\n",
       " 1.9730736017227173,\n",
       " 1.948495626449585,\n",
       " 1.9579170942306519,\n",
       " 1.9464221000671387,\n",
       " 1.9805620908737183,\n",
       " 1.985334873199463,\n",
       " 1.9667963981628418,\n",
       " 2.0034382343292236,\n",
       " 1.9491225481033325,\n",
       " 1.9679934978485107,\n",
       " 1.9633561372756958,\n",
       " 2.0184309482574463,\n",
       " 1.9360673427581787,\n",
       " 1.9817005395889282,\n",
       " 1.993281602859497,\n",
       " 1.9721715450286865,\n",
       " 1.9489269256591797,\n",
       " 1.943583369255066,\n",
       " 1.9927747249603271,\n",
       " 2.0007803440093994,\n",
       " 2.0244202613830566,\n",
       " 1.949920654296875,\n",
       " 1.9901381731033325,\n",
       " 1.892599105834961,\n",
       " 1.9550909996032715,\n",
       " 1.9602103233337402,\n",
       " 2.0106518268585205,\n",
       " 1.9637969732284546,\n",
       " 1.9899839162826538,\n",
       " 1.9881408214569092,\n",
       " 1.937881588935852,\n",
       " 1.9437427520751953,\n",
       " 1.9882804155349731,\n",
       " 1.9299975633621216,\n",
       " 1.9642752408981323,\n",
       " 1.945356845855713,\n",
       " 1.9849261045455933,\n",
       " 2.034686803817749,\n",
       " 1.9777377843856812,\n",
       " 1.9487866163253784,\n",
       " 1.9437834024429321,\n",
       " 1.931732177734375,\n",
       " 1.9870774745941162,\n",
       " 1.9739134311676025,\n",
       " 1.9736746549606323,\n",
       " 1.979524850845337,\n",
       " 1.9486770629882812,\n",
       " 1.99282968044281,\n",
       " 2.0159459114074707,\n",
       " 1.9460742473602295,\n",
       " 1.9672555923461914,\n",
       " 2.0035130977630615,\n",
       " 2.006868600845337,\n",
       " 1.9492746591567993,\n",
       " 1.9687138795852661,\n",
       " 1.9949398040771484,\n",
       " 1.9720830917358398,\n",
       " 1.9136103391647339,\n",
       " 1.9401156902313232,\n",
       " 1.9723187685012817,\n",
       " 1.9734266996383667,\n",
       " 1.8925707340240479,\n",
       " 1.9199198484420776,\n",
       " 1.9277721643447876,\n",
       " 1.9731874465942383,\n",
       " 1.9585658311843872,\n",
       " 1.9530187845230103,\n",
       " 1.9519892930984497,\n",
       " 1.9369983673095703,\n",
       " 1.9968392848968506,\n",
       " 1.965237021446228,\n",
       " 1.960639238357544,\n",
       " 1.980210304260254,\n",
       " 1.9784162044525146,\n",
       " 2.012441873550415,\n",
       " 1.9399123191833496,\n",
       " 1.971321702003479,\n",
       " 1.9114880561828613,\n",
       " 1.9409130811691284,\n",
       " 1.9803167581558228,\n",
       " 1.9050625562667847,\n",
       " 1.9346222877502441,\n",
       " 1.9397680759429932,\n",
       " 2.023604393005371,\n",
       " 1.9902878999710083,\n",
       " 1.9341332912445068,\n",
       " 1.890725016593933,\n",
       " 1.9303697347640991,\n",
       " 1.9157569408416748,\n",
       " 1.95404052734375,\n",
       " 1.8875470161437988,\n",
       " 1.9757918119430542,\n",
       " 1.9199131727218628,\n",
       " 1.9476205110549927,\n",
       " 1.8795714378356934,\n",
       " 1.9618709087371826,\n",
       " 1.9108412265777588,\n",
       " 1.960728645324707,\n",
       " 1.947768211364746,\n",
       " 1.920856237411499,\n",
       " 1.9052983522415161,\n",
       " 1.9043583869934082,\n",
       " 1.9373250007629395,\n",
       " 1.9203022718429565,\n",
       " 2.0139658451080322,\n",
       " 1.928676724433899,\n",
       " 1.914300560951233,\n",
       " 1.9357178211212158,\n",
       " 1.9412181377410889,\n",
       " 1.9265316724777222,\n",
       " 1.9040645360946655,\n",
       " 1.9102585315704346,\n",
       " 1.950410008430481,\n",
       " 1.8865573406219482,\n",
       " 1.8818740844726562,\n",
       " 1.9286277294158936,\n",
       " 1.9468228816986084,\n",
       " 1.9391425848007202,\n",
       " 1.903764247894287,\n",
       " 1.9442373514175415,\n",
       " 1.8750932216644287,\n",
       " 1.9453315734863281,\n",
       " 1.8754862546920776,\n",
       " 1.886786937713623,\n",
       " 1.90242600440979,\n",
       " 1.916741132736206,\n",
       " 1.864017128944397,\n",
       " 1.9741036891937256,\n",
       " 1.9048097133636475,\n",
       " 1.988452672958374,\n",
       " 1.9456896781921387,\n",
       " 1.9011650085449219,\n",
       " 1.9395807981491089,\n",
       " 1.921565294265747,\n",
       " 1.9232404232025146,\n",
       " 1.898361086845398,\n",
       " 1.936353325843811,\n",
       " 1.962428331375122,\n",
       " 1.8986916542053223,\n",
       " 1.9449599981307983,\n",
       " 1.9379469156265259,\n",
       " 1.9183415174484253,\n",
       " 1.9141769409179688,\n",
       " 1.9361639022827148,\n",
       " 1.9048144817352295,\n",
       " 1.9470754861831665,\n",
       " 1.9072082042694092,\n",
       " 1.9454096555709839,\n",
       " 1.900046467781067,\n",
       " 1.913914680480957,\n",
       " 1.949806571006775,\n",
       " 1.918361783027649,\n",
       " 1.8931243419647217,\n",
       " 1.8985581398010254,\n",
       " 1.8733108043670654,\n",
       " 1.8571239709854126,\n",
       " 1.9012831449508667,\n",
       " 1.8795808553695679,\n",
       " 1.8895173072814941,\n",
       " 1.9025622606277466,\n",
       " 1.8969041109085083,\n",
       " 1.9411358833312988,\n",
       " 1.9086709022521973,\n",
       " 1.9070792198181152,\n",
       " 1.952209234237671,\n",
       " 1.9383906126022339,\n",
       " 1.930198311805725,\n",
       " 1.9299687147140503,\n",
       " 1.9103924036026,\n",
       " 1.9459117650985718,\n",
       " 1.7995660305023193,\n",
       " 1.8526437282562256,\n",
       " 1.9465906620025635,\n",
       " 1.8553578853607178,\n",
       " 1.871790885925293,\n",
       " 1.9093403816223145,\n",
       " 1.9060368537902832,\n",
       " 1.8498010635375977,\n",
       " 1.9138352870941162,\n",
       " 1.9097522497177124,\n",
       " 1.8838075399398804,\n",
       " 1.8802847862243652,\n",
       " 1.8964948654174805,\n",
       " 1.889678955078125,\n",
       " 1.87662672996521,\n",
       " 1.8847547769546509,\n",
       " 1.9088976383209229,\n",
       " 1.9113996028900146,\n",
       " 1.8752342462539673,\n",
       " 1.938535451889038]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, losses=[]):\n",
    "    model.train()  # set model to training mode\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Forward pass\n",
    "        # TODO: perform the forward pass \n",
    "        pred = model.forward(X)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Just for logging\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # 1. Reset gradients\n",
    "        loss.backward()       # 2. Compute current gradients\n",
    "        optimizer.step()      # 3. Update parameters\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss_val = loss.item()\n",
    "            print(f\"loss: {loss_val:>7f}\")\n",
    "    return losses\n",
    "\n",
    "train(train_dataloader,model,loss_fn,optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d6f624",
   "metadata": {},
   "source": [
    "**5.2 Define a function to evaluate the model on the test dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a49fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    \"\"\"\n",
    "    TODO:  Evaluate the model on the test dataset and return the accuracy and average loss.\n",
    "    \"\"\"\n",
    "    model.eval()  # set model to eval mode\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Forward pass\n",
    "        # TODO: perform the forward pass \n",
    "        pred = model.forward(X)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Just for logging\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # 1. Reset gradients\n",
    "        loss.backward()       # 2. Compute current gradients\n",
    "        optimizer.step()      # 3. Update parameters\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss_val = loss.item()\n",
    "            print(f\"loss: {loss_val:>7f}\")\n",
    "    return losses\n",
    "\n",
    "accuracy, loss = test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3955ea92",
   "metadata": {},
   "source": [
    "### Saving and Loading Models in PyTorch\n",
    "\n",
    "After training a neural network, it’s important to save the learned parameters so we can reuse the model later without retraining from scratch.\n",
    "\n",
    "In PyTorch, we typically save the **state dictionary** (`state_dict`) of the model, which contains all trainable parameters (weights and biases).  \n",
    "\n",
    "```python\n",
    "torch.save(model.state_dict(), \"model_weights.pth\")\n",
    "```\n",
    "\n",
    "To reuse a saved model:\n",
    "\n",
    "1. Recreate the model architecture.\n",
    "2. Load the saved state dictionary into it.\n",
    "3. Evaluate or continue training as needed.\n",
    "\n",
    "This way, the new model has the same parameters as the trained one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f78a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model's parameters\n",
    "torch.save(model.state_dict(), \"model_weights.pth\")\n",
    "\n",
    "\n",
    "# Create a new instance of the model (untrained) and load the saved parameters\n",
    "model_new = NeuralNetwork()\n",
    "model_new.load_state_dict(torch.load(\"model_weights.pth\"))\n",
    "test(test_dataloader, model_new, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e51527",
   "metadata": {},
   "source": [
    "## 7. MCQ\n",
    "\n",
    "---\n",
    "\n",
    "## 7.1 Activation Functions\n",
    "Which of the following is the main purpose of using an activation function in a neural network?  \n",
    "\n",
    "A. To increase the number of layers in the network  \n",
    "B. To introduce non-linearity so the network can model complex functions  \n",
    "C. To normalize the input data before training  \n",
    "D. To reduce overfitting during training  \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "## 7.2. The Perceptron\n",
    "A single perceptron can only represent:  \n",
    "\n",
    "A. Any continuous function  \n",
    "B. Non-linear decision boundaries  \n",
    "C. Linear decision boundaries  \n",
    "D. Polynomial functions  \n",
    "\n",
    "**Answer:**\n",
    "\n",
    "---\n",
    "\n",
    "## 7.3. Linear Layer\n",
    "In a linear (fully connected) layer with input dimension $d$ and output dimension $m$, the weight matrix $W$ has the shape:  \n",
    "\n",
    "A. $(d \\times m)$  \n",
    "B. $(m \\times d)$  \n",
    "C. $(d \\times d)$  \n",
    "D. $(m \\times m)$  \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "## 7.4. Loss Functions\n",
    "The Mean Squared Error (MSE) loss between predictions $\\hat{y}$ and targets $y$ is defined as:  \n",
    "\n",
    "A. $\\frac{1}{N}\\sum_{i=1}^N |\\hat{y}_i - y_i|$  \n",
    "B. $\\frac{1}{N}\\sum_{i=1}^N (\\hat{y}_i - y_i)^2$  \n",
    "C. $\\sum_{i=1}^N (\\hat{y}_i - y_i)$  \n",
    "D. $\\max(\\hat{y}_i, y_i)$  \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "## 7.5. Multi-Layer Perceptron (MLP)\n",
    "Compared to a single perceptron, a multi-layer perceptron can:  \n",
    "\n",
    "A. Only model linear functions  \n",
    "B. Model more complex, non-linear functions  \n",
    "C. Avoid the need for activation functions  \n",
    "D. Train without using backpropagation  \n",
    "\n",
    "**Answer:**\n",
    "\n",
    "---\n",
    "\n",
    "## 7.6. Training Procedure\n",
    "Which of the following is the correct order of steps in one training iteration?  \n",
    "\n",
    "A. Backward pass → Forward pass → Update weights  \n",
    "B. Forward pass → Compute loss → Backward pass → Update weights  \n",
    "C. Update weights → Forward pass → Compute loss → Backward pass  \n",
    "D. Forward pass → Update weights → Compute loss → Backward pass  \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
