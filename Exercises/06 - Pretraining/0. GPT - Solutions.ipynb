{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f3ce31",
   "metadata": {},
   "source": [
    "# Mini GPT: Building a Language Model from Scratch\n",
    "\n",
    "In this session, we will **build and train a simplified GPT-like model** step by step inspired by Karpathy's `nanoGPT`.\n",
    "\n",
    "By the end, you will understand the **core building blocks** behind modern large language models.\n",
    "\n",
    "### What to expect\n",
    "- Learn how text is converted into numbers the model can understand.  \n",
    "- Explore how Transformers work: **embeddings, self-attention, feedforward layers, and residual connections**.  \n",
    "- Put these pieces together into a small GPT model.  \n",
    "- Train the model on Shakespeare’s text.  \n",
    "- Generate new, Shakespeare-like text from scratch.  \n",
    "\n",
    "⚡ This is a **hands-on educational exercise**: the model is tiny compared to real GPTs, but it captures the **same core ideas**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cec0493",
   "metadata": {},
   "source": [
    "### Setup and Hyperparameters\n",
    "\n",
    "We start by importing **PyTorch** (`torch`) and some of its modules:\n",
    "\n",
    "- `torch.nn`: building blocks for neural networks.\n",
    "- `torch.nn.functional`: activation functions, loss functions, and other utilities.\n",
    "\n",
    "Then we define our **hyperparameters** — these control how the model is trained and how large it is:\n",
    "\n",
    "- `batch_size = 64`: how many training sequences we process at the same time.\n",
    "- `block_size = 256`: the maximum length of context (how many previous characters the model can \"see\" when predicting the next one).\n",
    "- `max_iters = 5000`: number of training steps.\n",
    "- `eval_interval = 500`: how often we evaluate model performance on validation data.\n",
    "- `learning_rate = 3e-4`: how fast the optimizer updates weights.\n",
    "- `device`: whether to use GPU (`cuda`) or CPU.\n",
    "- `eval_iters = 200`: number of mini-batches used to estimate loss during evaluation.\n",
    "- `n_embd = 384`: size of the embedding vectors (each token is represented by a vector of this length).\n",
    "- `n_head = 6`: number of self-attention heads in the Transformer.\n",
    "- `n_layer = 6`: number of Transformer blocks stacked together.\n",
    "- `dropout = 0.2`: fraction of neurons randomly “dropped” during training to prevent overfitting.\n",
    "\n",
    "These values define both the **capacity** of the model and the **training process**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d84e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size   = 64    # Number of sequences processed in parallel\n",
    "block_size   = 256   # Maximum context length for predictions\n",
    "max_iters    = 5000  # Total number of training iterations\n",
    "eval_interval = 500  # Interval for evaluation\n",
    "learning_rate = 3e-4\n",
    "eval_iters    = 200  # Number of iterations to estimate loss\n",
    "\n",
    "n_embd  = 384   # Embedding dimension\n",
    "n_head  = 6     # Number of attention heads\n",
    "n_layer = 6     # Number of transformer blocks\n",
    "dropout = 0.2   # Dropout rate\n",
    "\n",
    "# Device configuration\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f94e0da",
   "metadata": {},
   "source": [
    "### Loading and Preparing the Dataset\n",
    "\n",
    "1. **Random seed**  \n",
    "   `torch.manual_seed(1337)` ensures reproducibility — we’ll get the same random numbers each run.\n",
    "\n",
    "2. **Load text**  \n",
    "   We read Shakespeare’s works (`input.txt`) into a single string called `text`.\n",
    "\n",
    "3. **Vocabulary**  \n",
    "   - `chars`: all unique characters that appear in the dataset.  \n",
    "   - `vocab_size`: total number of unique characters.  \n",
    "\n",
    "4. **Character ↔ Integer mapping**  \n",
    "   - `stoi` (“string to integer”): maps each character to an index.  \n",
    "   - `itos` (“integer to string”): reverse mapping.  \n",
    "   - `encode(s)`: converts a string into a list of integers.  \n",
    "   - `decode(l)`: converts a list of integers back into text.\n",
    "\n",
    "   > This is how we turn raw text into numerical data that a neural network can process.\n",
    "\n",
    "5. **Dataset split**  \n",
    "   - Encode the entire text into a `torch.tensor` of integers.  \n",
    "   - Use the first 90% as `train_data`.  \n",
    "   - Keep the last 10% as `val_data` (to measure generalization).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2abbbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tiny shakespeare dataset\n",
    "# !curl -O https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# OR\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Get all unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Create character ↔ integer mappings\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Encoder: string → list of integers\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "\n",
    "# Decoder: list of integers → string\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "# Train/validation split\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))  # 90% train, 10% validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3f3d20",
   "metadata": {},
   "source": [
    "### Creating Training Batches\n",
    "\n",
    "`get_batch(split)` samples random chunks of text:\n",
    "\n",
    "- Picks `batch_size` random starting positions.  \n",
    "- Builds input `x` (current characters) and target `y` (the same sequence shifted by one character).  \n",
    "- Moves tensors to the right device (CPU/GPU).\n",
    "\n",
    "Result: `x, y` have shape `(batch_size, block_size)` and are used to train the model to predict the **next character**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986882b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "def get_batch(split):\n",
    "    \"\"\"Generate a small batch of inputs (x) and targets (y).\"\"\"\n",
    "    \n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    \n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8b922f",
   "metadata": {},
   "source": [
    "### Estimating Loss\n",
    "\n",
    "`estimate_loss()` checks how well the model is doing without updating weights:\n",
    "\n",
    "- Disables gradient tracking (`@torch.no_grad()` → faster, less memory).  \n",
    "- Switches model to evaluation mode (`model.eval()` disables dropout).  \n",
    "- Runs `eval_iters` batches for both **train** and **val** sets.  \n",
    "- Collects and averages the losses.  \n",
    "- Switches back to training mode (`model.train()`).  \n",
    "\n",
    "This gives a stable estimate of training vs validation performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ec63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    \"\"\"Estimate the average loss for train and validation splits.\"\"\"\n",
    "    \n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        \n",
    "        for _ in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            logits, loss = model(x, y)\n",
    "            losses[_] = loss.item()\n",
    "        \n",
    "        out[split] = losses.mean()\n",
    "    \n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0bb7b7",
   "metadata": {},
   "source": [
    "### Self-Attention Head\n",
    "\n",
    "A single head of self-attention:\n",
    "\n",
    "- Projects input `x` into **queries**, **keys**, and **values**.  \n",
    "- Computes attention scores (`q @ k^T`), scaled for stability.  \n",
    "- Applies a **causal mask** (`tril`) so tokens can’t look ahead.  \n",
    "- Softmax → probabilities over past positions.  \n",
    "- Uses these to weight the values → output.\n",
    "\n",
    "Input shape: `(batch, time, channels)`  \n",
    "Output shape: `(batch, time, head_size)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, head_size: int):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        # Lower-triangular mask for causal self-attention\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, T, C)\n",
    "               B = batch size, T = time steps, C = embedding channels\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (B, T, head_size)\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Project to key, query, and value\n",
    "        k = self.key(x)    # (B, T, head_size)\n",
    "        q = self.query(x)  # (B, T, head_size)\n",
    "\n",
    "        # Compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)  # (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # Weighted aggregation of values\n",
    "        v = self.value(x)  # (B, T, head_size)\n",
    "        out = wei @ v      # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b6a57c",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "Instead of one head, we use several in parallel:\n",
    "\n",
    "- Each `Head` learns different attention patterns.  \n",
    "- Their outputs are concatenated (`torch.cat`) along the channel dimension.  \n",
    "- A final linear projection mixes them back into the embedding size (`n_embd`).  \n",
    "- Dropout is applied for regularization.\n",
    "\n",
    "This allows the model to attend to different types of relationships at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c384e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads: int, head_size: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, T, C)\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (B, T, n_embd)\n",
    "        \"\"\"\n",
    "        # Concatenate results from all attention heads\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "        # Final linear projection + dropout\n",
    "        out = self.dropout(self.proj(out))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7050f5",
   "metadata": {},
   "source": [
    "### FeedForward Layer\n",
    "\n",
    "A position-wise MLP applied to each token:\n",
    "\n",
    "- Expands embedding size (`n_embd → 4*n_embd`).  \n",
    "- Applies ReLU non-linearity.  \n",
    "- Projects back down to `n_embd`.  \n",
    "- Adds dropout for regularization.\n",
    "\n",
    "This gives the model extra capacity beyond attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ada9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"A simple feed-forward network: Linear → ReLU → Linear → Dropout.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, T, n_embd)\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (B, T, n_embd)\n",
    "        \"\"\"\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca679d8",
   "metadata": {},
   "source": [
    "### Transformer Block\n",
    "\n",
    "Each block combines **communication (attention)** and **computation (MLP)**:\n",
    "\n",
    "1. Apply LayerNorm → Multi-Head Attention → add residual connection.  \n",
    "2. Apply LayerNorm → FeedForward → add residual connection.  \n",
    "\n",
    "This structure lets tokens share information while keeping stable gradients.  \n",
    "Stacking several blocks builds the full Transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155f0f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: self-attention (communication) followed by feed-forward (computation).\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, n_head: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_embd: Embedding dimension\n",
    "            n_head: Number of attention heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, T, n_embd)\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (B, T, n_embd)\n",
    "        \"\"\"\n",
    "        # Apply pre-norm, self-attention, and residual connection\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "\n",
    "        # Apply pre-norm, feed-forward, and residual connection\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bd96d9",
   "metadata": {},
   "source": [
    "### GPTLanguageModel\n",
    "\n",
    "This class assembles the full Transformer-based language model:\n",
    "\n",
    "- **Token embeddings**: turn character indices into vectors.  \n",
    "- **Position embeddings**: add information about order in the sequence.  \n",
    "- **Stack of Transformer blocks**: attention + feedforward layers.  \n",
    "- **Final LayerNorm + Linear head**: map hidden states to vocabulary logits.  \n",
    "\n",
    "**Forward pass**:\n",
    "1. Look up token + position embeddings → combine them.  \n",
    "2. Pass through Transformer blocks.  \n",
    "3. Project to logits over the vocabulary.  \n",
    "4. If targets are provided, compute cross-entropy loss.  \n",
    "\n",
    "Output: `(logits, loss)` where  \n",
    "- `logits`: predictions for next token.  \n",
    "- `loss`: training signal (or `None` if not given).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21102d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    \"\"\"A simple GPT-style language model.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Token and positional embeddings\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "\n",
    "        # Final layer normalization and output head\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        \"\"\"Custom weight initialization.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(\n",
    "        self, idx: torch.Tensor, targets: torch.Tensor | None = None\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor | None]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx: Input indices of shape (B, T)\n",
    "            targets: Optional target indices of shape (B, T)\n",
    "\n",
    "        Returns:\n",
    "            logits: Predictions of shape (B, T, vocab_size)\n",
    "            loss: Cross-entropy loss (if targets provided), else None\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Token + positional embeddings\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "\n",
    "        # Transformer blocks and final norm\n",
    "        x = self.blocks(x)  # (B, T, C)\n",
    "        x = self.ln_f(x)    # (B, T, C)\n",
    "\n",
    "        # Output projection\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        # Compute loss if targets are provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    def generate(self, idx: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate new tokens autoregressively.\n",
    "\n",
    "        Args:\n",
    "            idx: Input indices of shape (B, T), the current context.\n",
    "            max_new_tokens: Number of tokens to generate.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (B, T + max_new_tokens) containing the original\n",
    "            indices followed by the generated tokens.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            # Forward pass through the model\n",
    "            logits, _ = self(idx_cond)\n",
    "\n",
    "            # Focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, vocab_size)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "\n",
    "            # Append sampled token to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f012b164",
   "metadata": {},
   "source": [
    "### Model Initialization and Optimizer\n",
    "\n",
    "- Create the model (`GPTLanguageModel`) and move it to the right device (CPU/GPU).  \n",
    "- Print the total number of parameters (in millions) → shows model size.  \n",
    "- Define the optimizer: **AdamW**, a variant of Adam with weight decay, commonly used for Transformers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd242a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and move to device\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# Print number of parameters (in millions)\n",
    "num_params = sum(p.numel() for p in m.parameters())\n",
    "print(f\"{num_params / 1e6:.2f}M parameters\")\n",
    "\n",
    "# Create PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e916ad",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "For `max_iters` steps:\n",
    "\n",
    "1. **Evaluate periodically**: every `eval_interval` steps (or at the end), estimate train/val loss.  \n",
    "2. **Get a batch**: `(xb, yb)` input and target sequences.  \n",
    "3. **Forward pass**: compute `logits` and `loss`.  \n",
    "4. **Backward pass**:  \n",
    "   - Reset gradients (`optimizer.zero_grad`).  \n",
    "   - Backpropagate (`loss.backward()`).  \n",
    "   - Update weights (`optimizer.step()`).  \n",
    "\n",
    "This loop gradually teaches the model to predict the next character.\n",
    "\n",
    "**Important Note:** This cell will require a lot (like a lot) of time to finish on a CPU. At this point you'll need a GPU to move on. The easiest way to access one is to simply upload this notebook (as it is) in your Google Drive and open the notebook in Google Colab. Then, you can navigate to `Runtime` -> `Change Runtime Type` and select a T4 GPU. No subscription, payment or anything else is required. Also don't forget to upload `input.txt` as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d97ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(max_iters):\n",
    "\n",
    "    # Evaluate loss on train/val splits at regular intervals\n",
    "    if step % eval_interval == 0 or step == max_iters - 1:\n",
    "        losses = estimate_loss(model)\n",
    "        print(\n",
    "            f\"Step {step}: \"\n",
    "            f\"train loss {losses['train']:.4f}, \"\n",
    "            f\"val loss {losses['val']:.4f}\"\n",
    "        )\n",
    "\n",
    "    # Sample a batch of training data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # Forward pass and loss computation\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b41b937",
   "metadata": {},
   "source": [
    "### Text Generation\n",
    "\n",
    "- Start with a single token (`0`) as the initial context.  \n",
    "- Call `model.generate(...)` to autoregressively sample the next tokens, up to `max_new_tokens=500`.  \n",
    "- Decode the generated token IDs back into text and print it.  \n",
    "\n",
    "👉 The model produces new Shakespeare-like text, character by character.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8df84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "\n",
    "# Short sample (print to console)\n",
    "generated = m.generate(context, max_new_tokens=500)\n",
    "print(decode(generated[0].tolist()))\n",
    "\n",
    "# Long sample (save to file)\n",
    "# with open(\"more.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(decode(m.generate(context, max_new_tokens=10_000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3f7bc1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## How This Mini GPT Differs from Real GPT Models\n",
    "\n",
    "Our model is inspired by GPT, but it’s much smaller and simpler.  \n",
    "Here are five key differences:\n",
    "\n",
    "1. **Scale**  \n",
    "   - GPT (e.g., GPT-3) has **billions of parameters**, trained on huge datasets with thousands of GPUs.  \n",
    "   - Our mini GPT has only a few **million parameters**, trained on a tiny dataset (Shakespeare) with a single GPU/CPU.\n",
    "\n",
    "2. **Tokenizer**  \n",
    "   - GPT uses **Byte Pair Encoding (BPE)** or similar subword tokenization → efficient and works for all languages.  \n",
    "   - Our version uses **character-level tokens** → simpler, but less efficient for large vocabularies.\n",
    "\n",
    "3. **Architecture Details**  \n",
    "   - GPT includes improvements like **pre-layer normalization**, **rotary embeddings**, and optimized attention implementations (e.g. FlashAttention).  \n",
    "   - Our mini GPT uses a **basic Transformer** with embeddings, attention, and feedforward layers.\n",
    "\n",
    "4. **Training Setup**  \n",
    "   - GPT is trained with **massive compute budgets** for weeks or months.  \n",
    "   - Our model trains in **hours** (or less) on a laptop GPU.\n",
    "\n",
    "5. **Purpose**  \n",
    "   - GPT is built for **general-purpose applications** (chat, coding, search, etc.).  \n",
    "   - Our mini GPT is designed for **learning and experimentation** — to show the core ideas in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e01225c",
   "metadata": {},
   "source": [
    "## MCQ\n",
    "\n",
    "### 1.1. Self-Attention vs RNNs  \n",
    "\n",
    "Which of the following is a major advantage of self-attention over RNNs?  \n",
    "\n",
    "A. Handles long-range dependencies better<br>  \n",
    "B. Requires less memory<br>  \n",
    "C. Avoids positional embeddings<br>  \n",
    "D. Uses fewer parameters<br>  \n",
    "\n",
    "**Answer:** A ✅  \n",
    "\n",
    "---\n",
    "\n",
    "### 1.2. Transformer Complexity  \n",
    "\n",
    "What is the typical disadvantage of transformer models compared to RNNs?  \n",
    "\n",
    "A. Difficulty handling variable-length sequences<br>  \n",
    "B. Quadratic computational complexity with sequence length<br>  \n",
    "C. Inability to model long dependencies<br>  \n",
    "D. Poor performance in machine translation<br>  \n",
    "\n",
    "**Answer:** B ✅  \n",
    "\n",
    "---\n",
    "\n",
    "### 1.3. Paradigm Shift in PLMs  \n",
    "\n",
    "Which of the following was a paradigm shift introduced by PLMs like GPT and BERT?  \n",
    "\n",
    "A. Training from scratch on small datasets<br>  \n",
    "B. Using pretrained embeddings (Word2Vec, GloVe) with RNNs<br>  \n",
    "C. Fine-tuning full pretrained models for downstream tasks<br>  \n",
    "D. Training only with unsupervised objectives<br>  \n",
    "\n",
    "**Answer:** C ✅  \n",
    "\n",
    "---\n",
    "\n",
    "### 1.4. Positional Embeddings  \n",
    "\n",
    "Why do transformers require positional embeddings?  \n",
    "\n",
    "A. To encode sequential order not captured by self-attention<br>  \n",
    "B. To reduce training data requirements<br>  \n",
    "C. To avoid vanishing gradients<br>  \n",
    "D. To replace token embeddings<br>  \n",
    "\n",
    "**Answer:** A ✅  \n",
    "\n",
    "---\n",
    "\n",
    "### 1.5. Benchmarks  \n",
    "\n",
    "Which benchmark highlighted the effectiveness of BERT in NLU tasks?  \n",
    "\n",
    "A. SQuAD<br>  \n",
    "B. GLUE<br>  \n",
    "C. MNIST<br>  \n",
    "D. CoNLL-2003<br>  \n",
    "\n",
    "**Answer:** B ✅  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.1. GPT Architecture  \n",
    "\n",
    "GPT uses which type of transformer architecture?  \n",
    "\n",
    "A. Encoder-only<br>  \n",
    "B. Decoder-only with masked self-attention<br>  \n",
    "C. Encoder-decoder with cross-attention<br>  \n",
    "D. LSTM-based encoder<br>  \n",
    "\n",
    "**Answer:** B ✅  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.2. GPT Objective  \n",
    "\n",
    "What was the main pretraining objective of GPT?  \n",
    "\n",
    "A. Next sentence prediction<br>  \n",
    "B. Masked language modeling<br>  \n",
    "C. Causal language modeling (next word prediction)<br>  \n",
    "D. Denoising autoencoding<br>  \n",
    "\n",
    "**Answer:** C ✅  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.3. GPT Dataset  \n",
    "\n",
    "GPT was first trained on which dataset?  \n",
    "\n",
    "A. Wikipedia + Toronto BooksCorpus<br>  \n",
    "B. OpenWebText<br>  \n",
    "C. Toronto BooksCorpus only<br>  \n",
    "D. Billion Word Benchmark<br>  \n",
    "\n",
    "**Answer:** C ✅  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.4. GPT vs GPT2  \n",
    "\n",
    "Which of the following was a major difference between GPT and GPT2?  \n",
    "\n",
    "A. GPT2 used masked LM<br>  \n",
    "B. GPT2 was trained on much larger data and had more parameters<br>  \n",
    "C. GPT2 introduced bidirectionality<br>  \n",
    "D. GPT2 was only for classification tasks<br>  \n",
    "\n",
    "**Answer:** B ✅  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.5. GPT Input Window  \n",
    "\n",
    "What was the main training window size for GPT’s input sequences?  \n",
    "\n",
    "A. 128 tokens<br>  \n",
    "B. 256 tokens<br>  \n",
    "C. 512 tokens<br>  \n",
    "D. 1024 tokens<br>  \n",
    "\n",
    "**Answer:** C ✅  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
