{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4146bf0b",
   "metadata": {},
   "source": [
    "# NLP with Hugging Face Transformers ðŸ§ ðŸ¤–\n",
    "\n",
    "In this notebook, you will get hands-on practice with two of the most widely used pre-trained models in Natural Language Processing (NLP): **BERT** and **T5**.  \n",
    "The goal is to familiarize yourself with how to load, tokenize, run inference, and decode predictions using the [ðŸ¤— Transformers library](https://huggingface.co/docs/transformers/index).\n",
    "\n",
    "\n",
    "## What to expect\n",
    "By completing these exercises, you will:\n",
    "- Understand how to use Hugging Face tokenizers and models.\n",
    "- Practice encoding raw text into model inputs.\n",
    "- Run models in inference mode and interpret their outputs.\n",
    "- Explore how masked language modeling (BERT) and sequence-to-sequence generation (T5) work.\n",
    "\n",
    "\n",
    "ðŸ‘‰ Work through the TODOs in each code cell, and run them to check your results.  \n",
    "Try experimenting with different input sentences/prompts once you get the basic version working!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4501a4",
   "metadata": {},
   "source": [
    "## Exercise 1: Masked Language Modeling with BERT\n",
    "BERT is a **bidirectional transformer** trained using a **masked language modeling** objective.  \n",
    "You will:\n",
    "- Load the pre-trained `bert-base-uncased` model and tokenizer.\n",
    "- Feed an input sentence containing a `[MASK]` token.\n",
    "- Predict the missing word using BERTâ€™s output logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d4d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load pre-trained BERT and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Input with a [MASK] token\n",
    "text = \"The capital of France is [MASK].\"\n",
    "\n",
    "# Encode input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Get prediction for [MASK]\n",
    "mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "predicted_word = tokenizer.decode(predicted_token_id)\n",
    "\n",
    "print(\"Input:    \", text)\n",
    "print(\"Prediction:\", predicted_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee22c3a2",
   "metadata": {},
   "source": [
    "## Exercise 2: Sequence-to-Sequence Generation with T5\n",
    "T5 (Text-to-Text Transfer Transformer) treats every NLP problem as a text-to-text task.  \n",
    "In this exercise, you will:\n",
    "- Load the pre-trained `t5-small` model and tokenizer.\n",
    "- Encode a translation prompt (English â†’ German).\n",
    "- Generate the translated sentence using T5â€™s sequence generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7451bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load pre-trained T5 and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Example: translation\n",
    "text = \"translate English to French: The house is wonderful.\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output sequence\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "# Decode to string\n",
    "print(\"Input:    \", text)\n",
    "print(\"Output:   \", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c743e5d8",
   "metadata": {},
   "source": [
    "## MCQ\n",
    "\n",
    "### 1.1. BART Hybrid Design  \n",
    "\n",
    "BART combines which two ideas?  \n",
    "\n",
    "A. Bidirectional encoder + Autoregressive decoder<br>  \n",
    "B. Masked LM + Next Sentence Prediction<br>  \n",
    "C. LSTM + CNN<br>  \n",
    "D. Word2Vec + Transformers<br>  \n",
    "\n",
    "**Answer:** A âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "### 1.2. BART vs BERT  \n",
    "\n",
    "Which of the following is TRUE about BART compared to BERT?  \n",
    "\n",
    "A. BART cannot handle generative tasks<br>  \n",
    "B. BART performs worse on classification tasks<br>  \n",
    "C. BART can do both understanding and generation<br>  \n",
    "D. BART uses only unidirectional encoding<br>  \n",
    "\n",
    "**Answer:** C âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "### 1.3. BART Architecture  \n",
    "\n",
    "What architecture does BART use?  \n",
    "\n",
    "A. Encoder-only<br>  \n",
    "B. Decoder-only<br>  \n",
    "C. Encoder-decoder with cross-attention<br>  \n",
    "D. RNN-based encoder<br>  \n",
    "\n",
    "**Answer:** C âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "### 1.4. BART Advantage  \n",
    "\n",
    "Why does BART outperform BERT on some tasks?  \n",
    "\n",
    "A. It has fewer parameters<br>  \n",
    "B. It was trained on significantly more data<br>  \n",
    "C. It does not use positional embeddings<br>  \n",
    "D. It uses static word embeddings<br>  \n",
    "\n",
    "**Answer:** B âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "### 1.5. BART Corruption Strategies  \n",
    "\n",
    "Which of the following corruption strategies can BART use during pretraining?  \n",
    "\n",
    "A. Masking tokens randomly<br>  \n",
    "B. Permuting sentences<br>  \n",
    "C. Replacing tokens with random ones<br>  \n",
    "D. All of the above<br>  \n",
    "\n",
    "**Answer:** D âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.1. T5 Full Form  \n",
    "\n",
    "What does T5 stand for?  \n",
    "\n",
    "A. Text-to-Text Transfer Transformer<br>  \n",
    "B. Transformer for 5 tasks<br>  \n",
    "C. Transferable Transformer Training Technique<br>  \n",
    "D. Text Transformer for Translation<br>  \n",
    "\n",
    "**Answer:** A âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.2. T5 Philosophy  \n",
    "\n",
    "What is the key design philosophy of T5?  \n",
    "\n",
    "A. Every NLP task can be framed as a text-to-text problem<br>  \n",
    "B. Using unidirectional LSTMs<br>  \n",
    "C. Using static embeddings<br>  \n",
    "D. Relying only on classification tasks<br>  \n",
    "\n",
    "**Answer:** A âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.3. T5 Pretraining  \n",
    "\n",
    "Which pretraining objective did T5 use?  \n",
    "\n",
    "A. Masked language modeling (like BERT)<br>  \n",
    "B. Next sentence prediction<br>  \n",
    "C. Denoising span corruption (span masking)<br>  \n",
    "D. Causal LM (like GPT)<br>  \n",
    "\n",
    "**Answer:** C âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.4. T5 Architecture  \n",
    "\n",
    "Which type of transformer architecture does T5 employ?  \n",
    "\n",
    "A. Encoder-only<br>  \n",
    "B. Decoder-only<br>  \n",
    "C. Encoder-decoder<br>  \n",
    "D. Hybrid LSTM-Transformer<br>  \n",
    "\n",
    "**Answer:** C âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.5. T5 Limitations  \n",
    "\n",
    "Which of the following is NOT a task T5 can handle natively in its framework?  \n",
    "\n",
    "A. Machine translation<br>  \n",
    "B. Summarization<br>  \n",
    "C. Image classification<br>  \n",
    "D. Question answering<br>  \n",
    "\n",
    "**Answer:** C âœ…  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
