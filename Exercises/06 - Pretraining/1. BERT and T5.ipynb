{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa5ba195",
   "metadata": {},
   "source": [
    "# NLP with Hugging Face Transformers ðŸ§ ðŸ¤–\n",
    "\n",
    "In this notebook, you will get hands-on practice with two of the most widely used pre-trained models in Natural Language Processing (NLP): **BERT** and **T5**.  \n",
    "The goal is to familiarize yourself with how to load, tokenize, run inference, and decode predictions using the [ðŸ¤— Transformers library](https://huggingface.co/docs/transformers/index).\n",
    "\n",
    "\n",
    "## What to expect\n",
    "By completing these exercises, you will:\n",
    "- Understand how to use Hugging Face tokenizers and models.\n",
    "- Practice encoding raw text into model inputs.\n",
    "- Run models in inference mode and interpret their outputs.\n",
    "- Explore how masked language modeling (BERT) and sequence-to-sequence generation (T5) work.\n",
    "\n",
    "\n",
    "ðŸ‘‰ Work through the TODOs in each code cell, and run them to check your results.  \n",
    "Try experimenting with different input sentences/prompts once you get the basic version working!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1787c664",
   "metadata": {},
   "source": [
    "## Exercise 1: Masked Language Modeling with BERT\n",
    "BERT is a **bidirectional transformer** trained using a **masked language modeling** objective.  \n",
    "You will:\n",
    "- Load the pre-trained `bert-base-uncased` model and tokenizer.\n",
    "- Feed an input sentence containing a `[MASK]` token.\n",
    "- Predict the missing word using BERTâ€™s output logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d4d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# TODO: Load pre-trained BERT tokenizer and model (\"bert-base-uncased\")\n",
    "tokenizer = ...\n",
    "model = ...\n",
    "\n",
    "# Example input with a [MASK] token\n",
    "text = \"The capital of France is [MASK].\"\n",
    "\n",
    "# TODO: Encode the input text into model inputs\n",
    "inputs = ...\n",
    "\n",
    "# TODO: Run the model in inference mode\n",
    "with torch.no_grad():\n",
    "    outputs = ...\n",
    "    logits = ...\n",
    "\n",
    "# TODO: Locate the index of the [MASK] token\n",
    "mask_token_index = ...\n",
    "\n",
    "# TODO: Select the most likely prediction for the [MASK]\n",
    "predicted_token_id = ...\n",
    "predicted_word = ...\n",
    "\n",
    "print(\"Input:    \", text)\n",
    "print(\"Prediction:\", predicted_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee22c3a2",
   "metadata": {},
   "source": [
    "## Exercise 2: Sequence-to-Sequence Generation with T5\n",
    "T5 (Text-to-Text Transfer Transformer) treats every NLP problem as a text-to-text task.  \n",
    "In this exercise, you will:\n",
    "- Load the pre-trained `t5-small` model and tokenizer.\n",
    "- Encode a translation prompt (English â†’ German).\n",
    "- Generate the translated sentence using T5â€™s sequence generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7451bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# TODO: Load pre-trained T5 tokenizer and model (\"t5-small\")\n",
    "tokenizer = ...\n",
    "model = ...\n",
    "\n",
    "# Example: translation task prompt\n",
    "text = \"translate English to German: The house is wonderful.\"\n",
    "\n",
    "# TODO: Encode input text for the model\n",
    "inputs = ...\n",
    "\n",
    "# TODO: Generate output sequence from the model\n",
    "outputs = ...\n",
    "\n",
    "# TODO: Decode the generated tokens into a string\n",
    "print(\"Input:    \", text)\n",
    "print(\"Output:   \", ...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e055b827",
   "metadata": {},
   "source": [
    "## MCQ \n",
    "\n",
    "### 1.1. BART Hybrid Design  \n",
    "\n",
    "BART combines which two ideas?  \n",
    "\n",
    "A. Bidirectional encoder + Autoregressive decoder<br>  \n",
    "B. Masked LM + Next Sentence Prediction<br>  \n",
    "C. LSTM + CNN<br>  \n",
    "D. Word2Vec + Transformers<br>  \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 1.2. BART vs BERT  \n",
    "\n",
    "Which of the following is TRUE about BART compared to BERT?  \n",
    "\n",
    "A. BART cannot handle generative tasks<br>  \n",
    "B. BART performs worse on classification tasks<br>  \n",
    "C. BART can do both understanding and generation<br>  \n",
    "D. BART uses only unidirectional encoding<br>  \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 1.3. BART Architecture  \n",
    "\n",
    "What architecture does BART use?  \n",
    "\n",
    "A. Encoder-only<br>  \n",
    "B. Decoder-only<br>  \n",
    "C. Encoder-decoder with cross-attention<br>  \n",
    "D. RNN-based encoder<br>  \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 1.4. BART Advantage  \n",
    "\n",
    "Why does BART outperform BERT on some tasks?  \n",
    "\n",
    "A. It has fewer parameters<br>  \n",
    "B. It was trained on significantly more data<br>  \n",
    "C. It does not use positional embeddings<br>  \n",
    "D. It uses static word embeddings<br>  \n",
    "\n",
    "**Answer:**\n",
    "\n",
    "---\n",
    "\n",
    "### 1.5. BART Corruption Strategies  \n",
    "\n",
    "Which of the following corruption strategies can BART use during pretraining?  \n",
    "\n",
    "A. Masking tokens randomly<br>  \n",
    "B. Permuting sentences<br>  \n",
    "C. Replacing tokens with random ones<br>  \n",
    "D. All of the above<br>  \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 2.1. T5 Full Form  \n",
    "\n",
    "What does T5 stand for?  \n",
    "\n",
    "A. Text-to-Text Transfer Transformer<br>  \n",
    "B. Transformer for 5 tasks<br>  \n",
    "C. Transferable Transformer Training Technique<br>  \n",
    "D. Text Transformer for Translation<br>  \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 2.2. T5 Philosophy  \n",
    "\n",
    "What is the key design philosophy of T5?  \n",
    "\n",
    "A. Every NLP task can be framed as a text-to-text problem<br>  \n",
    "B. Using unidirectional LSTMs<br>  \n",
    "C. Using static embeddings<br>  \n",
    "D. Relying only on classification tasks<br>  \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 2.3. T5 Pretraining  \n",
    "\n",
    "Which pretraining objective did T5 use?  \n",
    "\n",
    "A. Masked language modeling (like BERT)<br>  \n",
    "B. Next sentence prediction<br>  \n",
    "C. Denoising span corruption (span masking)<br>  \n",
    "D. Causal LM (like GPT)<br>  \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 2.4. T5 Architecture  \n",
    "\n",
    "Which type of transformer architecture does T5 employ?  \n",
    "\n",
    "A. Encoder-only<br>  \n",
    "B. Decoder-only<br>  \n",
    "C. Encoder-decoder<br>  \n",
    "D. Hybrid LSTM-Transformer<br>  \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 2.5. T5 Limitations  \n",
    "\n",
    "Which of the following is NOT a task T5 can handle natively in its framework?  \n",
    "\n",
    "A. Machine translation<br>  \n",
    "B. Summarization<br>  \n",
    "C. Image classification<br>  \n",
    "D. Question answering<br>  \n",
    "\n",
    "**Answer:**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
