{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f3ce31",
   "metadata": {},
   "source": [
    "# 📝 Mini GPT: Building a Language Model from Scratch\n",
    "\n",
    "Welcome! In this session, we’ll **build and train a simplified GPT-style model** inspired by Karpathy’s legendary [`nanoGPT`](https://github.com/karpathy/nanoGPT).  \n",
    "Think of this as a guided tour into the inner workings of modern large language models, but without needing a data center to run it. 🚀\n",
    "\n",
    "\n",
    "## What to expect\n",
    "- **Turn text into tokens**: see how raw text is mapped into numbers a model can process.  \n",
    "- **Peek inside Transformers**: embeddings, self-attention, feedforward layers, and residual connections.  \n",
    "- **Assemble a tiny GPT**: stack the building blocks into a working autoregressive model.  \n",
    "- **Train on Shakespeare’s text**: teach it the patterns of language.  \n",
    "- **Generate new text**: watch your model write like the Bard himself. ✍️\n",
    "\n",
    "\n",
    "\n",
    "💡 **Want a step-by-step walkthrough?**  \n",
    "Check out Andrej Karpathy’s fantastic [YouTube lecture](https://www.youtube.com/watch?v=kCc8FmEb1nY), where he builds and explains GPT-like models from scratch.  \n",
    "Our notebook complements his video; you’ll be coding along and experimenting directly.\n",
    "\n",
    "\n",
    "👉 By the end, you won’t just *use* a language model; you’ll actually **understand how one is built.**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cec0493",
   "metadata": {},
   "source": [
    "### Setup and Hyperparameters\n",
    "\n",
    "We start by importing **PyTorch** (`torch`) and some of its modules:\n",
    "\n",
    "- `torch.nn`: building blocks for neural networks.\n",
    "- `torch.nn.functional`: activation functions, loss functions, and other utilities.\n",
    "\n",
    "Then we define our **hyperparameters** — these control how the model is trained and how large it is:\n",
    "\n",
    "- `batch_size = 64`: how many training sequences we process at the same time.\n",
    "- `block_size = 256`: the maximum length of context (how many previous characters the model can \"see\" when predicting the next one).\n",
    "- `max_iters = 5000`: number of training steps.\n",
    "- `eval_interval = 500`: how often we evaluate model performance on validation data.\n",
    "- `learning_rate = 3e-4`: how fast the optimizer updates weights.\n",
    "- `device`: whether to use GPU (`cuda`) or CPU.\n",
    "- `eval_iters = 200`: number of mini-batches used to estimate loss during evaluation.\n",
    "- `n_embd = 384`: size of the embedding vectors (each token is represented by a vector of this length).\n",
    "- `n_head = 6`: number of self-attention heads in the Transformer.\n",
    "- `n_layer = 6`: number of Transformer blocks stacked together.\n",
    "- `dropout = 0.2`: fraction of neurons randomly “dropped” during training to prevent overfitting.\n",
    "\n",
    "These values define both the **capacity** of the model and the **training process**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1d84e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size   = 64    # Number of sequences processed in parallel\n",
    "block_size   = 256   # Maximum context length for predictions\n",
    "max_iters    = 5000  # Total number of training iterations\n",
    "eval_interval = 500  # Interval for evaluation\n",
    "learning_rate = 3e-4\n",
    "eval_iters    = 200  # Number of iterations to estimate loss\n",
    "\n",
    "n_embd  = 384   # Embedding dimension\n",
    "n_head  = 6     # Number of attention heads\n",
    "n_layer = 6     # Number of transformer blocks\n",
    "dropout = 0.2   # Dropout rate\n",
    "\n",
    "# Device configuration\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f94e0da",
   "metadata": {},
   "source": [
    "### Loading and Preparing the Dataset\n",
    "\n",
    "1. **Random seed**  \n",
    "   `torch.manual_seed(1337)` ensures reproducibility — we’ll get the same random numbers each run.\n",
    "\n",
    "2. **Load text**  \n",
    "   We read Shakespeare’s works (`input.txt`) into a single string called `text`.\n",
    "\n",
    "3. **Vocabulary**  \n",
    "   - `chars`: all unique characters that appear in the dataset.  \n",
    "   - `vocab_size`: total number of unique characters.  \n",
    "\n",
    "4. **Character ↔ Integer mapping**  \n",
    "   - `stoi` (“string to integer”): maps each character to an index.  \n",
    "   - `itos` (“integer to string”): reverse mapping.  \n",
    "   - `encode(s)`: converts a string into a list of integers.  \n",
    "   - `decode(l)`: converts a list of integers back into text.\n",
    "\n",
    "   > This is how we turn raw text into numerical data that a neural network can process.\n",
    "\n",
    "5. **Dataset split**  \n",
    "   - Encode the entire text into a `torch.tensor` of integers.  \n",
    "   - Use the first 90% as `train_data`.  \n",
    "   - Keep the last 10% as `val_data` (to measure generalization).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d41d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Download the dataset manually if not already present.\n",
    "# The file should be called \"input.txt\".\n",
    "# !curl -O https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# OR\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# TODO: Get all unique characters in the text\n",
    "chars = ...\n",
    "vocab_size = ...\n",
    "\n",
    "# TODO: Create two dictionaries for character ↔ integer mappings\n",
    "# - stoi (string to integer)\n",
    "# - itos (integer to string)\n",
    "stoi = ...\n",
    "itos = ...\n",
    "\n",
    "# TODO: Implement encoder (string → list of integers)\n",
    "encode = ...\n",
    "\n",
    "# TODO: Implement decoder (list of integers → string)\n",
    "decode = ...\n",
    "\n",
    "# Encode the entire dataset into integers\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# TODO: Split data into train (90%) and validation (10%)\n",
    "n = ...\n",
    "train_data = ...\n",
    "val_data = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3f3d20",
   "metadata": {},
   "source": [
    "### Creating Training Batches\n",
    "\n",
    "`get_batch(split)` samples random chunks of text:\n",
    "\n",
    "- Picks `batch_size` random starting positions.  \n",
    "- Builds input `x` (current characters) and target `y` (the same sequence shifted by one character).  \n",
    "- Moves tensors to the right device (CPU/GPU).\n",
    "\n",
    "Result: `x, y` have shape `(batch_size, block_size)` and are used to train the model to predict the **next character**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986882b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "def get_batch(split):\n",
    "    \"\"\"Generate a small batch of inputs (x) and targets (y).\"\"\"\n",
    "    \n",
    "    # TODO: Select the correct dataset depending on split (\"train\" or \"val\")\n",
    "    data = ...\n",
    "\n",
    "    # TODO: Sample random starting indices for each sequence in the batch\n",
    "    ix = ...\n",
    "\n",
    "    # TODO: Construct the input sequences (x) and target sequences (y)\n",
    "    # - x should be data[i : i + block_size]\n",
    "    # - y should be data[i + 1 : i + block_size + 1]\n",
    "    x = ...\n",
    "    y = ...\n",
    "\n",
    "    # TODO: Move to the correct device (e.g., \"cpu\" or \"cuda\")\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8b922f",
   "metadata": {},
   "source": [
    "### Estimating Loss\n",
    "\n",
    "`estimate_loss()` checks how well the model is doing without updating weights:\n",
    "\n",
    "- Disables gradient tracking (`@torch.no_grad()` → faster, less memory).  \n",
    "- Switches model to evaluation mode (`model.eval()` disables dropout).  \n",
    "- Runs `eval_iters` batches for both **train** and **val** sets.  \n",
    "- Collects and averages the losses.  \n",
    "- Switches back to training mode (`model.train()`).  \n",
    "\n",
    "This gives a stable estimate of training vs validation performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ec63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    \"\"\"Estimate the average loss for train and validation splits.\"\"\"\n",
    "    \n",
    "    out = {}\n",
    "    \n",
    "    # TODO: Put the model in evaluation mode\n",
    "    ...\n",
    "    \n",
    "    for split in [\"train\", \"val\"]:\n",
    "        # TODO: create a tensor to store losses for eval_iters runs\n",
    "        losses = ...\n",
    "        \n",
    "        for k in range(eval_iters):\n",
    "            # TODO: sample a batch of data\n",
    "            x, y = ...\n",
    "            \n",
    "            # TODO: run the model forward and compute the loss\n",
    "            logits, loss = ...\n",
    "            \n",
    "            # TODO: store the loss value\n",
    "            losses[k] = ...\n",
    "        \n",
    "        # TODO: compute the mean loss for this split\n",
    "        out[split] = ...\n",
    "    \n",
    "    # TODO: switch model back to training mode\n",
    "    ...\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0bb7b7",
   "metadata": {},
   "source": [
    "### Self-Attention Head\n",
    "\n",
    "A single head of self-attention:\n",
    "\n",
    "- Projects input `x` into **queries**, **keys**, and **values**.  \n",
    "- Computes attention scores (`q @ k^T`), scaled for stability.  \n",
    "- Applies a **causal mask** (`tril`) so tokens can’t look ahead.  \n",
    "- Softmax → probabilities over past positions.  \n",
    "- Uses these to weight the values → output.\n",
    "\n",
    "Input shape: `(batch, time, channels)`  \n",
    "Output shape: `(batch, time, head_size)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, head_size: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Define linear layers for key, query, and value projections\n",
    "        self.key = ...\n",
    "        self.query = ...\n",
    "        self.value = ...\n",
    "        \n",
    "        # TODO: Register a lower-triangular mask for causal self-attention\n",
    "        self.register_buffer(\"tril\", ...)\n",
    "        \n",
    "        # TODO: Add dropout for regularization\n",
    "        self.dropout = ...\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, T, C)\n",
    "               B = batch size, T = time steps, C = embedding channels\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (B, T, head_size)\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # TODO: Compute key and query projections\n",
    "        k = ...\n",
    "        q = ...\n",
    "\n",
    "        # TODO: Compute attention scores (scaled dot-product)\n",
    "        # - multiply q and k^T\n",
    "        # - scale by 1/sqrt(head_size)\n",
    "        wei = ...\n",
    "\n",
    "        # TODO: Apply the causal mask (use tril)\n",
    "        wei = ...\n",
    "\n",
    "        # TODO: Normalize scores with softmax and apply dropout\n",
    "        wei = ...\n",
    "\n",
    "        # TODO: Compute value projections and weighted sum\n",
    "        v = ...\n",
    "        out = ...\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b6a57c",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "Instead of one head, we use several in parallel:\n",
    "\n",
    "- Each `Head` learns different attention patterns.  \n",
    "- Their outputs are concatenated (`torch.cat`) along the channel dimension.  \n",
    "- A final linear projection mixes them back into the embedding size (`n_embd`).  \n",
    "- Dropout is applied for regularization.\n",
    "\n",
    "This allows the model to attend to different types of relationships at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c384e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads: int, head_size: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: create a list of attention heads\n",
    "        self.heads = ...\n",
    "        \n",
    "        # TODO: final projection layer (maps back to n_embd)\n",
    "        self.proj = ...\n",
    "        \n",
    "        # TODO: dropout for regularization\n",
    "        self.dropout = ...\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, T, C)\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (B, T, n_embd)\n",
    "        \"\"\"\n",
    "        # TODO: forward pass through each head and concatenate along the last dimension\n",
    "        out = ...\n",
    "\n",
    "        # TODO: apply final projection + dropout\n",
    "        out = ...\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7050f5",
   "metadata": {},
   "source": [
    "### FeedForward Layer\n",
    "\n",
    "A position-wise MLP applied to each token:\n",
    "\n",
    "- Expands embedding size (`n_embd → 4*n_embd`).  \n",
    "- Applies ReLU non-linearity.  \n",
    "- Projects back down to `n_embd`.  \n",
    "- Adds dropout for regularization.\n",
    "\n",
    "This gives the model extra capacity beyond attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ada9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"A simple feed-forward network: Linear → ReLU → Linear → Dropout.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: build a sequential module with:\n",
    "        # 1. Linear (n_embd → 4 * n_embd)\n",
    "        # 2. ReLU\n",
    "        # 3. Linear (4 * n_embd → n_embd)\n",
    "        # 4. Dropout\n",
    "        self.net = ...\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, T, n_embd)\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (B, T, n_embd)\n",
    "        \"\"\"\n",
    "        # TODO: forward pass through the network\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca679d8",
   "metadata": {},
   "source": [
    "### Transformer Block\n",
    "\n",
    "Each block combines **communication (attention)** and **computation (MLP)**:\n",
    "\n",
    "1. Apply LayerNorm → Multi-Head Attention → add residual connection.  \n",
    "2. Apply LayerNorm → FeedForward → add residual connection.  \n",
    "\n",
    "This structure lets tokens share information while keeping stable gradients.  \n",
    "Stacking several blocks builds the full Transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155f0f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: self-attention (communication) followed by feed-forward (computation).\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, n_head: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_embd: Embedding dimension\n",
    "            n_head: Number of attention heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        head_size = n_embd // n_head\n",
    "        \n",
    "        # TODO: Multi-head self-attention\n",
    "        self.sa = ...\n",
    "        \n",
    "        # TODO: Feed-forward network\n",
    "        self.ffwd = ...\n",
    "        \n",
    "        # TODO: Two layer normalizations\n",
    "        self.ln1 = ...\n",
    "        self.ln2 = ...\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, T, n_embd)\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (B, T, n_embd)\n",
    "        \"\"\"\n",
    "        # TODO: Apply pre-norm + self-attention + residual connection\n",
    "        x = ...\n",
    "        \n",
    "        # TODO: Apply pre-norm + feed-forward + residual connection\n",
    "        x = ...\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bd96d9",
   "metadata": {},
   "source": [
    "### GPTLanguageModel\n",
    "\n",
    "This class assembles the full Transformer-based language model:\n",
    "\n",
    "- **Token embeddings**: turn character indices into vectors.  \n",
    "- **Position embeddings**: add information about order in the sequence.  \n",
    "- **Stack of Transformer blocks**: attention + feedforward layers.  \n",
    "- **Final LayerNorm + Linear head**: map hidden states to vocabulary logits.  \n",
    "\n",
    "**Forward pass**:\n",
    "1. Look up token + position embeddings → combine them.  \n",
    "2. Pass through Transformer blocks.  \n",
    "3. Project to logits over the vocabulary.  \n",
    "4. If targets are provided, compute cross-entropy loss.  \n",
    "\n",
    "Output: `(logits, loss)` where  \n",
    "- `logits`: predictions for next token.  \n",
    "- `loss`: training signal (or `None` if not given).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21102d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    \"\"\"A simple GPT-style language model.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Token and positional embeddings\n",
    "        self.token_embedding_table = nn.Embedding(..., ...)\n",
    "        self.position_embedding_table = nn.Embedding(..., ...)\n",
    "\n",
    "        # TODO: Transformer blocks (stack of Block layers)\n",
    "        self.blocks = ...\n",
    "\n",
    "        # TODO: Final layer normalization and language modeling head\n",
    "        self.ln_f = ...\n",
    "        self.lm_head = ...\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        \"\"\"Custom weight initialization.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(\n",
    "        self, idx: torch.Tensor, targets: torch.Tensor | None = None\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor | None]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx: Input indices of shape (B, T)\n",
    "            targets: Optional target indices of shape (B, T)\n",
    "\n",
    "        Returns:\n",
    "            logits: Predictions of shape (B, T, vocab_size)\n",
    "            loss: Cross-entropy loss (if targets provided), else None\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # TODO: Compute token and positional embeddings, then add them\n",
    "        tok_emb = ...\n",
    "        pos_emb = ...\n",
    "        x = ...\n",
    "\n",
    "        # TODO: Pass through Transformer blocks and final layer norm\n",
    "        x = ...\n",
    "        x = ...\n",
    "\n",
    "        # TODO: Project to vocabulary size (output projection)\n",
    "        logits = ...\n",
    "\n",
    "        # TODO: Compute cross-entropy loss if targets are provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = ...\n",
    "            targets = ...\n",
    "            loss = ...\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate new tokens autoregressively.\n",
    "\n",
    "        Args:\n",
    "            idx: Input indices of shape (B, T), the current context.\n",
    "            max_new_tokens: Number of tokens to generate.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (B, T + max_new_tokens) containing the original\n",
    "            indices followed by the generated tokens.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # TODO: crop idx to the last block_size tokens (context window)\n",
    "            idx_cond = ...\n",
    "\n",
    "            # TODO: forward pass through the model\n",
    "            logits, _ = ...\n",
    "\n",
    "            # TODO: take only the logits for the last time step\n",
    "            logits = ...\n",
    "\n",
    "            # TODO: convert logits to probabilities with softmax\n",
    "            probs = ...\n",
    "\n",
    "            # TODO: sample the next token from the probability distribution\n",
    "            idx_next = ...\n",
    "\n",
    "            # TODO: append sampled token to the running sequence\n",
    "            idx = ...\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f012b164",
   "metadata": {},
   "source": [
    "### Model Initialization and Optimizer\n",
    "\n",
    "- Create the model (`GPTLanguageModel`) and move it to the right device (CPU/GPU).  \n",
    "- Print the total number of parameters (in millions) → shows model size.  \n",
    "- Define the optimizer: **AdamW**, a variant of Adam with weight decay, commonly used for Transformers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd242a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and move to device\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# Print number of parameters (in millions)\n",
    "num_params = sum(p.numel() for p in m.parameters())\n",
    "print(f\"{num_params / 1e6:.2f}M parameters\")\n",
    "\n",
    "# Create PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e916ad",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "For `max_iters` steps:\n",
    "\n",
    "1. **Evaluate periodically**: every `eval_interval` steps (or at the end), estimate train/val loss.  \n",
    "2. **Get a batch**: `(xb, yb)` input and target sequences.  \n",
    "3. **Forward pass**: compute `logits` and `loss`.  \n",
    "4. **Backward pass**:  \n",
    "   - Reset gradients (`optimizer.zero_grad`).  \n",
    "   - Backpropagate (`loss.backward()`).  \n",
    "   - Update weights (`optimizer.step()`).  \n",
    "\n",
    "This loop gradually teaches the model to predict the next character.\n",
    "\n",
    "**Important Note:** This cell will require a lot (like a lot) of time to finish on a CPU. At this point you'll need a GPU to move on. The easiest way to access one is to simply upload this notebook (as it is) in your Google Drive and open the notebook in Google Colab. Then, you can navigate to `Runtime` -> `Change Runtime Type` and select a T4 GPU. No subscription, payment or anything else is required. Also don't forget to upload `input.txt` as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d97ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(max_iters):\n",
    "\n",
    "    # Evaluate loss on train/val splits at regular intervals\n",
    "    if step % eval_interval == 0 or step == max_iters - 1:\n",
    "        losses = estimate_loss(model)\n",
    "        print(\n",
    "            f\"Step {step}: \"\n",
    "            f\"train loss {losses['train']:.4f}, \"\n",
    "            f\"val loss {losses['val']:.4f}\"\n",
    "        )\n",
    "\n",
    "    # Sample a batch of training data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # Forward pass and loss computation\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b41b937",
   "metadata": {},
   "source": [
    "### Text Generation\n",
    "\n",
    "- Start with a single token (`0`) as the initial context.  \n",
    "- Call `model.generate(...)` to autoregressively sample the next tokens, up to `max_new_tokens=500`.  \n",
    "- Decode the generated token IDs back into text and print it.  \n",
    "\n",
    "👉 The model produces new Shakespeare-like text, character by character.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8df84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "\n",
    "# Short sample (print to console)\n",
    "generated = m.generate(context, max_new_tokens=500)\n",
    "print(decode(generated[0].tolist()))\n",
    "\n",
    "# Long sample (save to file)\n",
    "# with open(\"more.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(decode(m.generate(context, max_new_tokens=10_000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3f7bc1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## How This Mini GPT Differs from Real GPT Models\n",
    "\n",
    "Our model is inspired by GPT, but it’s much smaller and simpler.  \n",
    "Here are five key differences:\n",
    "\n",
    "1. **Scale**  \n",
    "   - GPT (e.g., GPT-3) has **billions of parameters**, trained on huge datasets with thousands of GPUs.  \n",
    "   - Our mini GPT has only a few **million parameters**, trained on a tiny dataset (Shakespeare) with a single GPU/CPU.\n",
    "\n",
    "2. **Tokenizer**  \n",
    "   - GPT uses **Byte Pair Encoding (BPE)** or similar subword tokenization → efficient and works for all languages.  \n",
    "   - Our version uses **character-level tokens** → simpler, but less efficient for large vocabularies.\n",
    "\n",
    "3. **Architecture Details**  \n",
    "   - GPT includes improvements like **pre-layer normalization**, **rotary embeddings**, and optimized attention implementations (e.g. FlashAttention).  \n",
    "   - Our mini GPT uses a **basic Transformer** with embeddings, attention, and feedforward layers.\n",
    "\n",
    "4. **Training Setup**  \n",
    "   - GPT is trained with **massive compute budgets** for weeks or months.  \n",
    "   - Our model trains in **hours** (or less) on a laptop GPU.\n",
    "\n",
    "5. **Purpose**  \n",
    "   - GPT is built for **general-purpose applications** (chat, coding, search, etc.).  \n",
    "   - Our mini GPT is designed for **learning and experimentation** — to show the core ideas in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e01225c",
   "metadata": {},
   "source": [
    "## MCQ\n",
    "\n",
    "### 1.1. Self-Attention vs RNNs  \n",
    "\n",
    "Which of the following is a major advantage of self-attention over RNNs?  \n",
    "\n",
    "A. Handles long-range dependencies better<br>  \n",
    "B. Requires less memory<br>  \n",
    "C. Avoids positional embeddings<br>  \n",
    "D. Uses fewer parameters<br>  \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 1.2. Transformer Complexity  \n",
    "\n",
    "What is the typical disadvantage of transformer models compared to RNNs?  \n",
    "\n",
    "A. Difficulty handling variable-length sequences<br>  \n",
    "B. Quadratic computational complexity with sequence length<br>  \n",
    "C. Inability to model long dependencies<br>  \n",
    "D. Poor performance in machine translation<br>  \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 1.3. Paradigm Shift in PLMs  \n",
    "\n",
    "Which of the following was a paradigm shift introduced by PLMs like GPT and BERT?  \n",
    "\n",
    "A. Training from scratch on small datasets<br>  \n",
    "B. Using pretrained embeddings (Word2Vec, GloVe) with RNNs<br>  \n",
    "C. Fine-tuning full pretrained models for downstream tasks<br>  \n",
    "D. Training only with unsupervised objectives<br>  \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 1.4. Positional Embeddings  \n",
    "\n",
    "Why do transformers require positional embeddings?  \n",
    "\n",
    "A. To encode sequential order not captured by self-attention<br>  \n",
    "B. To reduce training data requirements<br>  \n",
    "C. To avoid vanishing gradients<br>  \n",
    "D. To replace token embeddings<br>  \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 1.5. Benchmarks  \n",
    "\n",
    "Which benchmark highlighted the effectiveness of BERT in NLU tasks?  \n",
    "\n",
    "A. SQuAD<br>  \n",
    "B. GLUE<br>  \n",
    "C. MNIST<br>  \n",
    "D. CoNLL-2003<br>  \n",
    "\n",
    "**Answer:**\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1. GPT Architecture  \n",
    "\n",
    "GPT uses which type of transformer architecture?  \n",
    "\n",
    "A. Encoder-only<br>  \n",
    "B. Decoder-only with masked self-attention<br>  \n",
    "C. Encoder-decoder with cross-attention<br>  \n",
    "D. LSTM-based encoder<br>  \n",
    "\n",
    "**Answer:**\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2. GPT Objective  \n",
    "\n",
    "What was the main pretraining objective of GPT?  \n",
    "\n",
    "A. Next sentence prediction<br>  \n",
    "B. Masked language modeling<br>  \n",
    "C. Causal language modeling (next word prediction)<br>  \n",
    "D. Denoising autoencoding<br>  \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 2.3. GPT Dataset  \n",
    "\n",
    "GPT was first trained on which dataset?  \n",
    "\n",
    "A. Wikipedia + Toronto BooksCorpus<br>  \n",
    "B. OpenWebText<br>  \n",
    "C. Toronto BooksCorpus only<br>  \n",
    "D. Billion Word Benchmark<br>  \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 2.4. GPT vs GPT2  \n",
    "\n",
    "Which of the following was a major difference between GPT and GPT2?  \n",
    "\n",
    "A. GPT2 used masked LM<br>  \n",
    "B. GPT2 was trained on much larger data and had more parameters<br>  \n",
    "C. GPT2 introduced bidirectionality<br>  \n",
    "D. GPT2 was only for classification tasks<br>  \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 2.5. GPT Input Window  \n",
    "\n",
    "What was the main training window size for GPT’s input sequences?  \n",
    "\n",
    "A. 128 tokens<br>  \n",
    "B. 256 tokens<br>  \n",
    "C. 512 tokens<br>  \n",
    "D. 1024 tokens<br>  \n",
    "\n",
    "**Answer:** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
